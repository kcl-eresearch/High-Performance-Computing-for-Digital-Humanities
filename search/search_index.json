{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"High performance computing for Digital Humanities","text":"<p>This workshop is an introduction to using High-Performance Computing (HPC) systems, using the King's College London CREATE HPC as an example.</p> <p>By the end of the workshop you should be able to:</p> <ul> <li>Find and load available software</li> <li>Use Python virtual environments</li> <li>Understand what a job scheduler does and why this is important for HPC systems</li> <li>Use job scheduler commands to submit jobs and find out information about them</li> <li>Submit different types of jobs</li> <li>Use Singularity containers</li> </ul>"},{"location":"#what-is-an-hpc-system","title":"What is an HPC system?","text":"<p>An HPC system is a term that usually describes a clustered network of computers. The computers in a cluster typically share a common purpose, and are used to accomplish tasks that might otherwise be too big for any one computer. Those computers might share the same, or similar hardware specifications, are usually connected via high speed networks and are backed by a fast, network filesystems.</p> <p>At a very high level, those computers can be divided into the following categories:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> <li>Storage nodes</li> <li>Management nodes</li> </ul> <p>Important</p> <p>Login nodes should only be used for submitting jobs and running simple tasks, such as editing of your job scripts, etc</p> <p></p>"},{"location":"#about-create","title":"About CREATE","text":"<p>King's Computational Research, Engineering and Technology Environment (CREATE) is a tightly integrated ecosystem of research computing infrastructure hosted by King\u2019s College London. It consists of:</p> <ul> <li>CREATE Cloud: A private cloud platform to provide flexible and scalable hosting environments, allowing researchers greater control over their own research computing resources using virtual machines</li> <li>CREATE HPC: A high performance compute cluster with CPU and GPU nodes, fast network interconnects and shared storage, for large scale simulations and data analytics</li> <li>CREATE RDS: A very large, highly resilient storage area for longer term curation of research data</li> <li>CREATE TRE: Tightly controlled project areas making use of Cloud and HPC resources to process sensitive datasets (e.g. clinical PIID) complying with NHS Digital audit standards (DSPT)</li> <li>CREATE Web: A self-service web hosting platform for static content (HTML/CSS/JS) and WordPress sites</li> </ul> <p>Important</p> <p>For research that has made use of CREATE, please see the Acknowledging page.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>In order to join this training workshop, you must have a terminal application installed on your computer. If you use MacOS or Linux, you will have one available by default. If you use Windows, you can use PowerShell - some commands will differ from those used on Mac/Linux.</p> <p>If you are working through these materials on your own, outside a workshop, you will also need to request an account on the CREATE HPC system. Information on how to request access can be found in the CREATE documentation</p>"},{"location":"#references","title":"References","text":"<p>The material in this course was inspired by / based on the following resources</p> <ul> <li>King's e-Research general Introduction to HPC workshop materials</li> <li>CREATE documentation</li> <li>EPCC Introduction to High-Performance Computing</li> <li>A previous iteration of this course developed at Maudsley BRC</li> </ul>"},{"location":"connecting/","title":"Before we begin","text":""},{"location":"connecting/#introduction-to-the-terminal","title":"Introduction to the terminal","text":"<p>Throughout this training we'll be interacting with CREATE via a terminal or command line. This means we'll be providing our instructions and receiving responses in text form. There's a few important commands we'll cover gradually as we need them.</p> <p>Why text instead of a graphical interface? Text-based interfaces were naturally the default before graphical interfaces became widely available, but even now they remain the most common way to interact with remote HPC systems. There are several advantages which a text-based interface gets us:</p> <ul> <li>They're precise - by using a well established set of commands it's clear what we mean</li> <li>They're repeatable - since our commands are just text, they can be easily saved, shared and run as automated scripts</li> <li>They're low-bandwidth - by using text instead of graphics, we're not taking up network bandwidth that could be better used for moving data around</li> </ul> <p>Depending on your operating system there are a number of terminal applications available which we would recommend:</p> <ul> <li>Windows: PowerShell - note that this uses a different command syntax, but once we're logged into CREATE this won't matter</li> <li>MacOS: Terminal</li> <li>Linux: The name of your terminal application may vary, but it's usually called something like \"Terminal\"</li> </ul> <p>You should be familiar with basic terminal commands before participating in this workshop. However, we have created a list of common commands you can use as a reference sheet.</p>"},{"location":"connecting/#connecting-to-create-hpc","title":"Connecting to CREATE HPC","text":"<p>You should have been provided with a guest username and password which you can use to log in to CREATE HPC. King's staff and students use their King's user id, or \"k-number\" to connect, so the example username we will use in this workshop is <code>k1234567</code>. You can connect to the login nodes using the following command, replacing <code>k1234567</code> with your guest username:</p> <pre><code>ssh k1234567@hpc.create.kcl.ac.uk\n</code></pre> <p>You will be prompted to enter your password:</p> <pre><code>$ ssh k1234567@hpc.create.kcl.ac.uk\nk1234567@hpc.create.kcl.ac.uk's password: \n</code></pre> <p>Type your password then press 'Enter'. You will not see any indication that your password has been typed in - this is normal! If the password is incorrect you will be prompted to retype it.</p> <p>Upon successful connection you should see something similar to</p> <pre><code>==================================================================\nKing's College London | e-Research\n------------------------------------------------------------------\n\nWelcome to erc-hpc-login1.create.kcl.ac.uk\n\n\nThis machine is part of\n\n \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\n\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d\n\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n------------------------------------------------------------------\nThis machine is automatically updated on the 5th of each month\nat 03:47 BST. It may reboot shortly thereafter.\n------------------------------------------------------------------\n\nDocumentation: https://docs.er.kcl.ac.uk/\n\nPlease contact e-research@kcl.ac.uk for support.\n...\nOnly files stored within /rds and /users are backed up.\n\nPlease remember to acknowledge CREATE in your research outputs:\n  https://docs.er.kcl.ac.uk/CREATE/acknowledging/\n\n==================================================================\nk1234567@erc-hpc-login1:~$\n</code></pre>"},{"location":"connecting/#the-environment","title":"The environment","text":"<ul> <li>Ubuntu 22.04 LTS Linux operating system with the Bash shell</li> <li>Directories/locations of interest<ul> <li>Home directory located in <code>/users/k-number</code></li> <li>Personal scratch space located in <code>/scratch/users/k-number</code></li> <li>Group scratch space located in <code>/scratch/grp/group-name</code></li> <li>Project space located in <code>/scratch/prj/prj-name</code></li> <li>RDS space located in <code>/rds/prj-name</code></li> <li>Datasets space located in <code>/datasets/dataset-name</code></li> </ul> </li> </ul> <p>Most of these directories will be useful to you at some point over the course of your research, so we'll briefly introduce each in turn. For more information, see our storage docs.</p> <p>Home directory: <code>/users/k-number</code>. Your home directory is the directory you see when you log in to CREATE. This is a good place to put any code you write or other custom software, configuration files and small amounts of data. This directory is only accessible by you by default - other users can't access files in your home directory unless you specifically allow them to. You will typically be allocated 50GiB for your home directory.</p> <p>Personal scratch: <code>/scratch/users/k-number</code>. Your personal scratch space is where you should store the input and output data for your active research. Scratch space is designed to provide much higher performance which is important for many HPC jobs and has a much higher capacity and personal allocation. Your personal scratch space can only be accessed by you by default - just like your home directory. You will typically be allocated 200GiB for your personal scratch, but may request more if necessary.</p> <p>Group scratch: <code>/scratch/grp/group-name</code> and Project scratch: <code>/scratch/prj/prj-name</code>. Group or project scratch space is similar to your personal scratch space but as the name suggests is shared by a group or a project. Every member of the group or project will be able to access the files stored in this space. Projects are initially allocated 1TiB, but may request more space if necessary at a cost of \u00a350 per TiB. Researchers may request that a project be created in CREATE for their work by emailing e-research@kcl.ac.uk.</p> <p>Research Data Store (RDS): <code>/rds/prj-name</code>.</p> <p>The Research Data Store is our large-scale storage facility for live research data. This is suitable for the largest datasets used by projects and where it's important that the data be properly backed up.</p> <p>Important</p> <p>The only data storage locations that are backed up are your home directory (<code>/users/k-number</code>) and RDS projects (<code>/rds/prj-name</code>). Data stored in scratch is not backed up, so make sure you copy any important results or code to RDS or another location that is backed up.</p>"},{"location":"exercises/","title":"Exercises","text":"<p>This section contains collection of exercises based on the material covered in the previous sections.</p> <p>Sample answers are provided in-line in the expandable boxes, but please try to do exercises by yourself before looking at the sample answers. There are also some hints to help you if you are stuck.</p> <p>Supporting files can be found on CREATE HPC in <code>/datasets/hpc_training</code> directory. The directory also contains example scripts from the exercises for convenience.</p> <p>Info</p> <p>There might be multiple ways to achieve a specific goal - If your method does not match the sample answer it might not mean you have done things incorrectly, just differently.</p>"},{"location":"exercises/#using-modules","title":"Using modules","text":""},{"location":"exercises/#basic-software-environment-module-usage","title":"Basic software environment module usage","text":"<p>Goal: Load and use the Python environment module with the most recent version of Python</p> <ul> <li>Investigate the module environment before and after loading the module. What has changed?</li> <li>Print python version</li> <li>Check the python interpreter location on the filesystem</li> <li>Unload the module and check the python version and location again. What is different?</li> </ul> Hint <p>Use <code>which</code> command to find the location and <code>-V</code> option to print python version.</p> Sample answer <p>First print the list of the modules before the module has been loaded</p> <pre><code>k1234567@erc-hpc-login2:~$ module list\n</code></pre> <p>Next load the Python module - N.B. the most recent version you have available may be different to this example answer.</p> <pre><code>k1234567@erc-hpc-login2:~$ module load python/3.8.12-gcc-9.4.0\n</code></pre> <p>List the currently loaded modules:</p> <pre><code>k1234567@erc-hpc-login2:~$ module list\n\nCurrently Loaded Modules:\n  1) bzip2/1.0.8-gcc-9.4.0     6) libiconv/1.16-gcc-9.4.0   11) ncurses/6.2-gcc-9.4.0             16) xz/5.2.5-gcc-9.4.0\n  2) libmd/1.0.3-gcc-9.4.0     7) libxml2/2.9.12-gcc-9.4.0  12) openssl/1.1.1l-gcc-9.4.0          17) zlib/1.2.11-gcc-9.4.0\n  3) libbsd/0.11.3-gcc-9.4.0   8) tar/1.34-gcc-9.4.0        13) readline/8.1-gcc-9.4.0            18) python/3.8.12-gcc-9.4.0\n  4) expat/2.4.1-gcc-9.4.0     9) gettext/0.21-gcc-9.4.0    14) sqlite/3.36.0-gcc-9.4.0\n  5) gdbm/1.19-gcc-9.4.0      10) libffi/3.3-gcc-9.4.0      15) util-linux-uuid/2.36.2-gcc-9.4.0\n</code></pre> <p>You can see that the python module and its dependencies have been added.</p> <p>Test by printing python version:</p> <pre><code>k1234567@erc-hpc-login2:~$ python -V\nPython 3.8.12\n</code></pre> <p>You can also see where the python interpreter resides:</p> <pre><code>k1234567@erc-hpc-login2:~$ which python\n/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/python-3.8.12-mdneme5mnx2ihlvy6ihbvjatdvnn45l6/bin/python\n</code></pre> <p>Unload the python module using:</p> <pre><code>k1234567@erc-hpc-login2:~$ module rm python/3.8.12-gcc-9.4.0\n</code></pre> <p>Check the version and location again:</p> <pre><code>k1234567@erc-hpc-login2:~$ python -V\nPython 3.8.10\nk1234567@erc-hpc-login2:~$ which python\n/usr/bin/python\n</code></pre> <p>Now we are using system provided python. Although we said that no software is automatically pre-loaded, there will be applications that come with the operating system and cannot be removed/hidden easily.</p>"},{"location":"exercises/#further-environment-module-usage","title":"Further environment module usage","text":"<p>Goal: Explore loading different versions of the same environment</p> <ul> <li>Load the most recent version of Python and then attempt to load a different version. What is the outcome?</li> </ul> Sample answer <p>Start by loading python 3.8.12 (N.B. the versions you have available may be different to this example answer):</p> <pre><code>k1234567@erc-hpc-login2:~$ module load python/3.8.12-gcc-9.4.0\n</code></pre> <p>Next load python 2.7.18:</p> <pre><code>k1234567@erc-hpc-login2:~$ module load python/2.7.18-gcc-9.4.0\n</code></pre> <p>You should see the following message informing you about the module swap:</p> <pre><code>The following have been reloaded with a version change:\n  1) python/3.8.12-gcc-9.4.0 =&gt; python/2.7.18-gcc-9.4.0\n</code></pre>"},{"location":"exercises/#python-virtual-environments","title":"Python virtual environments","text":""},{"location":"exercises/#python-virtualenv-creation","title":"Python virtualenv creation","text":"<p>Goal: Create and use a Python virtual environment</p> <ul> <li>Create python virtualenv using the most recent available version of python and activate it.</li> <li>Find out where the python interpreter and pip utility are located</li> <li>Deactivate the python virtual environment</li> <li>Check the location of python interpreter and pip utility again</li> </ul> Sample answer <p>Start by loading the python interpreter using:</p> <pre><code>k1234567@erc-hpc-login2:~$ module load python/3.11.6-gcc-13.2.0\n</code></pre> <p>Once the module has been loaded create the python virtual environment:</p> <pre><code>k1234567@erc-hpc-login2:~$ python -m venv myvenv\n</code></pre> <p>Next activate the environment:</p> <pre><code>k1234567@erc-hpc-login2:~$ source myvenv/bin/activate\n(myvenv) k1234567@erc-hpc-login2:~$\n</code></pre> <p>Use <code>which</code> command to print the locations of the python interpreter and pip utility</p> <pre><code>(myvenv) k1234567@erc-hpc-login2:~$ which python\n/users/k1234567/myvenv/bin/python\n(myvenv) k1234567@erc-hpc-login2:~$ which pip\n/users/k1234567/myvenv/bin/pip\n</code></pre> <p>Note: The above paths might vary depending on where on the filesystem you have created your virtual environemnt.</p> <p>Next deactivate the virtual environment</p> <pre><code>(myvenv) k1234567@erc-hpc-login2:~$ deactivate\n</code></pre> <p>Check the location of python interpreter and pip using</p> <pre><code>k1234567@erc-hpc-login2:~$ which python\n/software/spackages_v0_21_prod/apps/linux-ubuntu22.04-zen2/gcc-13.2.0/python-3.11.6-oe7bpykqsieymznu5rprjla46ti6uagh/bin/python\nk1234567@erc-hpc-login2:~$ which pip\n/usr/bin/pip\n</code></pre>"},{"location":"exercises/#python-virtualenv-package-installation","title":"Python virtualenv package installation","text":"<p>Goal: Install packages into a python virtual environment</p> <ul> <li>Using the virtualenv created in the previous excercise, install the <code>pandas</code> package to it. Check if it has been installed.</li> <li>What other packages were installed as dependencies?</li> </ul> Hint <ul> <li>Use <code>pip install package_name</code> to install a package in your virtual environment</li> <li>Use <code>pip list</code> to list installed packages</li> </ul> Sample answer <p>Using the previously created environment activate it</p> <pre><code>k1234567@erc-hpc-login2:~$ source myvenv/bin/activate\n(myvenv) k1234567@erc-hpc-login2:~$\n</code></pre> <p>Next use pip command to install the relevant package</p> <pre><code>(myvenv) k1234567@erc-hpc-login2:~$ pip install pandas\nCollecting pandas\n  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.7/11.7 MB 14.4 MB/s eta 0:00:00\nCollecting python-dateutil&gt;=2.8.1\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 247.7/247.7 KB 1.8 MB/s eta 0:00:00\nCollecting pytz&gt;=2020.1\n  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 503.5/503.5 KB 1.7 MB/s eta 0:00:00\nCollecting numpy&gt;=1.18.5\n  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.8/16.8 MB 13.6 MB/s eta 0:00:00\nCollecting six&gt;=1.5\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, six, numpy, python-dateutil, pandas\nSuccessfully installed numpy-1.22.3 pandas-1.4.2 python-dateutil-2.8.2 pytz-2022.1 six-1.16.0\n</code></pre> <p>Additional dependencies that pandas package requires will also be installed.</p> <p>You can check if the package has been installed using</p> <pre><code>(myvenv) k1234567@erc-hpc-login2:~$ pip list\nPackage         Version\n--------------- -------\nnumpy           1.22.3\npandas          1.4.2\npip             22.0.4\npython-dateutil 2.8.2\npytz            2022.1\nsetuptools      62.1.0\nsix             1.16.0\nwheel           0.37.1\n</code></pre> <p>Note: The version numbers you see might not match those above.</p>"},{"location":"exercises/#job-submission-part-1","title":"Job submission (part 1)","text":""},{"location":"exercises/#slurm-batch-job-submission","title":"SLURM batch job submission","text":"<p>Goal: Submit a batch job requesting 1 core and 2G of memory</p> <ul> <li>Have the job print the hostname of the node that it runs on</li> <li>Check if the job is running</li> <li>Analyse the output file</li> <li>Check information about completed job</li> </ul> Hint <ul> <li>Use <code>hostname</code> command to print the hostname of the node</li> <li>Add <code>sleep</code> to the jobscript so that it does not terminate too quickly</li> <li>Use <code>squeue</code> to check the status of the job</li> <li>Use <code>sacct</code> to get the information about the job when it has finished</li> </ul> Sample answer <p>Create a sample script <code>test-job1.sh</code> and add the following contents to it</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-job1\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2G\n\necho \"My hostname is \"`hostname`\nsleep 60\n</code></pre> <p>Submit the jobs using</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-job1.sh\nSubmitted batch job 56739\n</code></pre> <p>You can check the status of your job(s)</p> <pre><code>k1234567@erc-hpc-login2:~$ squeue --me\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n56739       cpu test-job k1234567  R       0:10      1 erc-hpc-comp001\n</code></pre> <p>Analyse the output, which should be located in the <code>slurm-jobid.out</code> file (replace <code>jobid</code> with the id of your job)</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56739.out\nMy hostname is erc-hpc-comp001\n</code></pre> <p>You can check information about completed job using</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 56739\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n56739         test-job1        cpu        kcl          1  COMPLETED      0:0\n56739.batch       batch                   kcl          1  COMPLETED      0:0\n</code></pre>"},{"location":"exercises/#slurm-interactive-job-submission","title":"SLURM interactive job submission","text":"<p>Goal: Request an interactive session with one core and 4G of memory</p> <ul> <li>Print the hostname of the interactive node that you have been allocated</li> <li>Print number of cores available</li> <li>What happens immediately after requesting the interactive job? How is it different from submitting a batch job? Why might this be a bad thing?</li> </ul> Hint <ul> <li>Use <code>hostname</code> command to print the hostname of the node</li> <li>Use <code>nproc</code> utility to print the number of allocated cpus for the job</li> </ul> Sample answer <p>Use <code>srun</code> command to request an interactive session</p> <pre><code>k1234567@erc-hpc-login2:~$ srun -p cpu --reservation cpu_introduction --mem 4G --pty /bin/bash -l\nsrun: job 56741 queued and waiting for resources\nsrun: job 56741 has been allocated resources\nk1234567@erc-hpc-comp001:~$\n</code></pre> <p>Print the hostname of the allocated node and number of cores that we have been allocated</p> <pre><code>k1234567@erc-hpc-comp001:~$ hostname\nerc-hpc-comp001\nk1234567@erc-hpc-comp001:~$ nproc\n1\n</code></pre>"},{"location":"exercises/#gpu-slurm-jobs","title":"GPU SLURM jobs","text":"<p>Goal: Submit a job requesting gpu(s) on a single node.</p> <ul> <li>Write a job script that requests a single gpu and does something that reports the information in its output</li> <li>Submit a job that requests two gpus and check the output</li> </ul> Hint <ul> <li>Use <code>nvidia-smi --id=$CUDA_VISIBLE_DEVICES</code> utility to print the details of allocated gpu(s) for the job</li> </ul> Sample answer <p>Create a sample script <code>test-gpu.sh</code> and add the following contents to it:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-gpu\n#SBATCH --partition=interruptible_gpu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --gres gpu:1\n\nnvidia-smi --id=$CUDA_VISIBLE_DEVICES\n</code></pre> <p>Submit the jobs using</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-gpu.sh\nSubmitted batch job 57425\n</code></pre> <p>Analyse the output, which should be located in the <code>slurm-jobid.out</code> file (replace <code>jobid</code> with the id of your job)</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-57425.out\nTue May 10 09:23:12 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   1  NVIDIA A100-SXM...  On   | 00000000:31:00.0 Off |                    0 |\n| N/A   42C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>Next modify <code>--gres</code> option to be <code>--gres gpu:2</code> and re-submit the job.</p> <p>Analyse the output, which should be located in the <code>slurm-jobid.out</code> file (replace <code>jobid</code> with the id of your job)</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-57460.out\nTue May 10 09:32:21 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   1  NVIDIA A100-SXM...  Off  | 00000000:31:00.0 Off |                    0 |\n| N/A   35C    P0    55W / 400W |      0MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A100-SXM...  Off  | 00000000:B1:00.0 Off |                    0 |\n| N/A   34C    P0    55W / 400W |      0MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"exercises/#debugging-slurm-job-scripts-1","title":"Debugging SLURM job scripts (1)","text":"<p>Goal: Analyse and fix a SLURM batch job script</p> <ul> <li>For this exercise, use the <code>bad-script1.sh</code> script located in the <code>sample-scripts</code> folder in the supporting files directory</li> <li>Examine the job script, what is it intended to do?</li> <li>Submit it as a batch job. What is the problem? Try to fix the issue.</li> <li>How else could you have fixed the issue?</li> </ul> Hint <p>You will have to copy the script to your local workspace in order to modify it.</p> Sample Answer <p>Submit the <code>/datasets/hpc_training/sample-scripts/sample-scripts/bad-script1.sh</code> job</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch /datasets/hpc_training/sample-scripts/sample-scripts/bad-script1.sh\n</code></pre> <p>Once the job has finished analyse its output file</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56749.out\nslurmstepd-erc-hpc-comp001: error: *** JOB 56749 ON erc-hpc-comp001 CANCELLED AT 2022-05-09T14:41:14 DUE TO TIME LIMIT ***\n</code></pre> <p>and the accounting information</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 56749\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n56749        bad-scrip+        cpu        kcl          1    TIMEOUT      0:0\n56749.batch       batch                   kcl          1  CANCELLED     0:15\n</code></pre> <p>You can see that the job was killed because insufficient runtime was requested. Fix it by changing the runtime to 3 minutes which should conver the 2 minute sleep (You can alternatively decrease the sleep time).</p> <pre><code>#SBATCH -t 0-0:03 # time (D-HH:MM)\n</code></pre> <p>Once the amended script is submitted the output file should contain the following</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56751.out\nHello World! erc-hpc-comp001\n</code></pre> <p>and the accouting information should contain</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 56751\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n56751        bad-scrip+        cpu        kcl          1  COMPLETED      0:0\n56751.batch       batch                   kcl          1  COMPLETED      0:0\n</code></pre>"},{"location":"exercises/#debugging-slurm-job-scripts-2","title":"Debugging SLURM job scripts (2)","text":"<p>Goal: Analyse and fix a SLURM batch job script</p> <ul> <li>For this exercise, use the <code>bad-script2.sh</code> script located in the <code>sample-scripts</code> folder in the supporting files directory</li> <li>Examine the job script, what resources does it request from SLURM?</li> <li>Submit it as a batch job. What is the problem? Try to fix the issue.</li> <li>How else could you have fixed the issue?</li> </ul> Hint <p>You will have to copy the script to your local workspace in order to modify it.</p> Sample answer <p>Submit the <code>/datasets/hpc_training/sample-scripts/bad-script2.sh</code> job</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch /datasets/hpc_training/sample-scripts/bad-script2.sh\n</code></pre> <p>Once the job has finished analyse its output file</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-12345.out\nSettings:\nmemset_increment_size =&gt; 0.0\nmemset_size =&gt; 2\nmemset_sleep =&gt; 10\nmemalloc_sleep =&gt; 0\ncpuburn =&gt; 1\nmemalloc =&gt; 1\nburntime =&gt; 20\nnprocs =&gt; 1\nmemalloc_size =&gt; 2\nncores =&gt; 1\n...\nslurmstepd: error: Detected 1 oom-kill event(s) in step 12345.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.\n</code></pre> <p>and the accounting information:</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 12345\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n12345        bad-scrip+        cpu    default          1 OUT_OF_ME+    0:125\n12345   .ba+      batch               default          1 OUT_OF_ME+    0:125\n</code></pre> <p>This time the job was killed because of insufficient memory. Fix it by changing the required memory to 3G, or by decreasing the amount of memory needed by cpumemburn.</p> <pre><code>#SBATCH --mem=3G\n</code></pre> <p>Once the amended script is submitted the output file should contain the following:</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-57619.out\nSettings:\nmemset_increment_size =&gt; 0.0\nmemset_size =&gt; 2\nmemset_sleep =&gt; 10\nmemalloc_sleep =&gt; 0\ncpuburn =&gt; 1\nmemalloc =&gt; 1\nburntime =&gt; 20\nnprocs =&gt; 1\nmemalloc_size =&gt; 2\nncores =&gt; 1\n...\n</code></pre> <p>and the accounting information should contain:</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 57619\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n57619        bad-scrip+        cpu    default          1  COMPLETED      0:0\n57619.ba+         batch               default          1  COMPLETED      0:0\n</code></pre>"},{"location":"exercises/#job-submission-part-2","title":"Job submission (part 2)","text":""},{"location":"exercises/#multi-core-slurm-jobs","title":"Multi-core SLURM jobs","text":"<p>Goal: Submit a job requesting multiple cores on a single node.</p> <ul> <li>Write a job script that requests two cores and does something that reports the information in its output</li> <li>Submit the job and check whether the correct resources have been allocated to the job by SLURM. Does it match the output from your job?</li> <li>How else could you have achieved the same resource request?</li> </ul> Hint <ul> <li>Use <code>nproc</code> utility to print the number of allocated cpus for the job</li> <li>Use <code>sacct</code> to check the resource allocation after the job has finished</li> </ul> Sample answer <p>Create a sample script <code>test-multicore.sh</code> and add the following contents to it:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-multicore\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n\necho \"I have \"`nproc`\" cpus.\"\n</code></pre> <p>Submit the jobs using:</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-multicore.sh\nSubmitted batch job 56758\n</code></pre> <p>Analyse the output, which should be located in the <code>slurm-jobid.out</code> file (replace <code>jobid</code> with the id of your job)</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56758.out\nI have 2 cpus.\n</code></pre> <p>You can check the information about completed job using</p> <pre><code>k1234567@erc-hpc-login2:~$ sacct -j 56758\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n56758        test-mult+        cpu        kcl          2  COMPLETED      0:0\n56758.batch       batch                   kcl          2  COMPLETED      0:0\n</code></pre> <p>You should see the <code>AllocCPUS</code> with value of 2.</p>"},{"location":"exercises/#slurm-array-jobs","title":"SLURM array jobs","text":"<p>Goal: Submit an array job consisting of 3 tasks</p> <ul> <li>Write a job script that defines an array of 3 tasks</li> <li>Have your job print out the SLURM array task ID number in its output</li> <li>Submit the script and then immediately check the status of the running job. How many individual jobs are running?</li> <li>How do you identify array jobs and tasks in the queue displayed by SLURM?</li> <li>Investigate the output files - how does an array job differ from the other jobs that you have been submitting?</li> </ul> Hint <ul> <li>Use <code>sleep</code> command to delay the execution of the script so that you can check its status</li> <li>Use <code>squeue</code> command to check the status of the running job.</li> <li>Use <code>SLURM_ARRAY_TASK_ID</code> environment variable inside the job to get the task id associated witht he task.</li> </ul> Sample answer <p>Create a sample script <code>test-array.sh</code> and add the following contents to it:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-array\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --array=1-3\n\nsleep 60\necho \"My task id is ${SLURM_ARRAY_TASK_ID}\"\n</code></pre> <p>Submit the jobs using</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-array.sh\nSubmitted batch job 56759\n</code></pre> <p>Check the status of the job using:</p> <pre><code>k1234567@erc-hpc-login2:~$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           56759_1       cpu test-arr k1234567  R       0:12      1 erc-hpc-comp001\n           56759_2       cpu test-arr k1234567  R       0:12      1 erc-hpc-comp001\n           56759_3       cpu test-arr k1234567  R       0:12      1 erc-hpc-comp001\n</code></pre> <p>You may see separate entries corresponding to each of the tasks in your array job.</p> <p>Analyse the output, which should be located in the <code>slurm-jobid_X.out</code> files (replace <code>jobid</code> with the id of your job). <code>X</code> will be a number corresponding to your tasks range, so you should have 3 separate log files, one for each array task.</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56759_1.out\nMy task id is 1\nk1234567@erc-hpc-login2:~$ cat slurm-56759_2.out\nMy task id is 2\nk1234567@erc-hpc-login2:~$ cat slurm-56759_3.out\nMy task id is 3\n</code></pre>"},{"location":"exercises/#organising-input-for-slurm-array-jobs","title":"Organising input for SLURM array jobs","text":"<p>Goal: Define an array job where each task takes its input from a different file</p> <ul> <li>Write an array job script where each task will print the contents of the files located in the <code>sample-files/arrayjob</code> folder in the supporting files directory</li> <li>Each task should only print the contents of one file, no two tasks should print the same file</li> <li>Submit the job, confirm the contents of the array tasks output files match the input file contents as intended</li> </ul> Hint <ul> <li>Try to find the association between the task id and the filename</li> <li>Use <code>cat</code> to print out the contents of the file</li> </ul> Sample answer <p>Create a sample script <code>test-array-files.sh</code> and add the following contents to it</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-array\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --array=1-3\n\ncat /datasets/hpc_training/sample-files/arrayjob/file${SLURM_ARRAY_TASK_ID}.txt\n</code></pre> <p>Submit the jobs using</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-array-files.sh\nSubmitted batch job 56769\n</code></pre> <p>Analyse the output, which should be located in the <code>slurm-jobid_X.out</code> files (replace <code>jobid</code> with the id of your job). <code>X</code> will be a number corresponding to your tasks range, so you should have 3 separate log files, one for each array task.</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56769_1.out\nHello, I'm file 1\nk1234567@erc-hpc-login2:~$ cat slurm-56769_2.out\nHello, I'm file 2\nk1234567@erc-hpc-login2:~$ cat slurm-56769_3.out\nHello, I'm file 3  \n</code></pre>"},{"location":"exercises/#using-jupyter","title":"Using Jupyter","text":""},{"location":"exercises/#jupyter-labnotebook-jobs","title":"Jupyter Lab/Notebook jobs","text":"<p>Goal: Submit a Jupyter lab notebook job and connect to it using SSH tunnelling</p> <ul> <li>Jupyter Lab should be installed in a Python virtual environment</li> <li>Sample <code>test-jlab.sh</code> script is located in the <code>sample-scripts</code> folder in the supporting files directory. You won't have to change any existing content but you will have to add some things in</li> </ul> Hint <ul> <li>You will have to copy the script to your local workspace in order to modify it</li> <li>Create python virtual environment and install <code>jupyterlab</code> package into it</li> <li>Once the job is running the output file will give you the ssh tunnel detils</li> </ul> Sample answer <p>First of all we need to create the python virtualenv and install jupyter lab package:</p> <pre><code>k1234567@erc-hpc-login2:~$ module load python/3.8.12-gcc-9.4.0\nk1234567@erc-hpc-login2:~$ python -m venv jvenv\nk1234567@erc-hpc-login2:~$ source jvenv/bin/activate\n(jvenv) k1234567@erc-hpc-login2:~$ pip install jupyterlab\nCollecting jupyterlab\n...\nSuccessfully installed MarkupSafe ...\n(jvenv) k1234567@erc-hpc-login2:~$ deactivate\n</code></pre> <p>Next we copy the <code>test-jlab.sh</code> and modify it. The only two things that need to be added are:</p> <ul> <li>Module loading of the python module</li> <li>Virtualenv activation before the <code>jupyter-lab</code> command is executed</li> </ul> <p>After the modifications the script might look like:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-jlab\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --mem=2G\n#SBATCH --signal=USR2\n#SBATCH --cpus-per-task=1\n\nmodule load python/3.8.12-gcc-9.4.0\n\n# get unused socket per https://unix.stackexchange.com/a/132524\nreadonly IPADDRESS=$(hostname -I | tr ' ' '\\n' | grep '10.211.4.')\nreadonly PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\ncat 1&gt;&amp;2 &lt;&lt;END\n1. SSH tunnel from your workstation using the following command:\n\n   ssh -NL 8888:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8888\n\nWhen done using the notebook, terminate the job by\nissuing the following command on the login node:\n\n      scancel -f ${SLURM_JOB_ID}\n\nEND\n\nsource jvenv/bin/activate\njupyter-lab --port=${PORT} --ip=${IPADDRESS} --no-browser\n\nprintf 'notebook exited' 1&gt;&amp;2\n</code></pre> <p>Next submit the job and wait until its running</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-jlab.sh\n</code></pre> <p>Once the job has started look at the output file - at the top of the file there will be instructions on how to connect to the instance from you laptop/workstation using ssh tunnel.</p> <p>After the connection instructions, there will be jupyter lab specific output. You can ignore most of it with the exception of the token that you will need to authenticate to the web interface.</p>"},{"location":"exercises/#singularity","title":"Singularity","text":""},{"location":"exercises/#singularity-container-usage","title":"Singularity container usage","text":"<p>Goal: Pull and run a container</p> <ul> <li>Pull the <code>lolcow</code> Singularity container and run it. What is the output?</li> </ul> Hint <ul> <li>Start an interactive session before running the commands</li> <li>Use <code>singularity pull</code> command to download the container</li> <li>Use <code>singularity run</code> command to run the container</li> </ul> Sample answer <p>Start an interactive session first as singularity is not available on the login nodes</p> <pre><code>srun --partition cpu --reservation cpu_introduction --pty /bin/bash -l\n</code></pre> <p>After the resources have been allocated, pull the lolcow container using <code>singularity pull</code></p> <pre><code>k1234567@erc-hpc-comp179:~$ singularity pull docker://sylabsio/lolcow\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\n...\nINFO:    Creating SIF file...\n</code></pre> <p>Once the container has been downloaded use <code>run</code> command to run it</p> <pre><code>k1234567@erc-hpc-comp179:~$ singularity run lolcow_latest.sif\nINFO:    Converting SIF file to temporary sandbox...\n ________________________________________\n/ Alas, how love can trifle with itself! \\\n|                                        |\n| -- William Shakespeare, \"The Two       |\n\\ Gentlemen of Verona\"                   /\n ----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"exercises/#using-containers-in-a-batch-job","title":"Using containers in a batch job","text":"<p>Goal: Submit a batch job and execute singularity container command</p> <ul> <li>Submit the batch job using default scheduler options</li> <li>Use <code>lolcow</code> container</li> <li>Execute <code>cowsay</code> command within the container that prints <code>Hello there</code> string</li> </ul> Hint <ul> <li>Use <code>cowsay string</code> command to print <code>string</code> text</li> <li>Ensure that you use the correct path to the container file</li> </ul> Sample answer <p>Create a sample script <code>test-singularity.sh</code> and add the following contents to it</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=test-singularity\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n\nsingularity exec ~/lolcow_latest.sif cowsay \"Hello there\"\n</code></pre> <p>Submit the jobs using</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch test-singularity.sh\nSubmitted batch job 56748\n</code></pre> <p>Analyse the output, which should be located in the <code>slurm-jobid.out</code> file (replace <code>jobid</code> with the id of your job)</p> <pre><code>k1234567@erc-hpc-login2:~$ cat slurm-56748.out\n _____________\n&lt; Hello there &gt;\n -------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"exercises/#finding-containers-that-are-useful-for-your-research","title":"Finding containers that are useful for your research","text":"<p>Goal: Use DockerHub to find a container that could be useful for your research and run it with Singularity</p> <p>DockerHub is an online repository of container images. Many commonly used pieces of software have been containerized into images that are officially endorsed, providing a convenient way to use this software without needing to install it.</p> <ul> <li>On DockerHub, search for some software you use in your research and see if you can find a container for that software</li> <li>Can you find documentation about how to use the container?</li> <li>Download the image to the HPC cluster</li> <li> <p>Run it with Singularity</p> Hint <ul> <li>If you can't think of a software to search for, try searching for <code>R</code> or <code>python</code>.</li> <li>Start an interactive session before running the commands</li> <li>Use <code>singularity pull docker://{container name}</code> command to download the container (replacing the <code>{}</code> and their contents with the correct container name)</li> <li>Use <code>singularity run</code> command to run the container</li> </ul> Sample answer <p>Let's search for a container that provides the <code>tidyverse</code> R packages.</p> <p>There is a <code>tidyverse</code> container provided by the <code>rocker</code> organisation.</p> <p>To pull the container, first start an interactive session as singularity is not available on the login nodes:</p> <pre><code>srun --partition cpu --reservation cpu_introduction --pty /bin/bash -l\n</code></pre> <p>We can then pull the container with:</p> <pre><code>singularity pull tidyverse.sif docker://rocker/tidyverse\n</code></pre> <p>We can run R using the container with:</p> <pre><code>singularity exec tidyverse.sif R\n</code></pre> </li> </ul>"},{"location":"jupyter/","title":"Jupyter notebooks","text":"<p>Jupyter notebooks provide a convenient way to run Python code and visualise outputs through a graphical user interface.</p> <p>HPC clusters typically don't provide access to a graphical user interface - all interactions happen through the terminal. However, there are ways to run software with a graphical user interface on an HPC cluster. This allows you to benefit from the resources available on the HPC (multiple cores for parallel jobs, more memory than available on your computer) while also having the convenience of a familiar graphical user interface.</p>"},{"location":"jupyter/#ssh-tunneling","title":"SSH tunneling","text":"<p>The approach we will use to connect to the Jupyter notebook today is called SSH tunneling (or SSH port forwarding). A port, in this context, is a virtual point where network connections between devices start and end. Computers have 65,535 possible ports; each port is assigned a number to identify it. Some port numbers are reserved to identify specific services. For example, port 80 is used for HTTP connections and port 443 for HTTPS.</p> <p>Jupyter notebooks use port 8888 by default - if you run a Jupyter notebook locally e.g. using <code>jupyterlab</code>, you likely access it by going to <code>localhost:8888/lab</code> in your web browser.</p> <p>To connect to a Jupyter notebook (or other service) running on an HPC cluster, we have to set up an SSH tunnel that connects a port on our local machine to the port used by the Jupyter notebook on the HPC node.</p> <p>We can do this using the <code>ssh</code> command with specific options:</p> <pre><code>ssh -NL 8888:10.211.123.123:12345 k1234567@hpc.create.kcl.ac.uk\n</code></pre> <ul> <li>The <code>-N</code> and <code>-L</code> options specify that we want to just set up port forwarding and not run any commands on the HPC node.</li> <li><code>8888</code> is the local port we want to use to connect to the Jupyter notebook</li> <li><code>10.211.123.123</code> is the IP address of the node we want to connect to</li> <li><code>12345</code> is the port on that node that the Jupyter notebook is running on (when running on a shared system, it's best not to use the default <code>8888</code> port as someone else might also be using it!)</li> </ul>"},{"location":"jupyter/#installing-jupyterlab","title":"Installing Jupyterlab","text":"<p>We will use <code>jupyterlab</code> to run a Jupyter notebook on the HPC. We'll create a new Python virtual environment for this, but note that you could also install the <code>jupyterlab</code> package within an existing virtual environment if you have created a virtual env for a specific project.</p> <pre><code>module load python/3.11.6-gcc-13.2.0\npython -m venv jupyter_env\n</code></pre> <p>We'll activate the environment and install the <code>jupyterlab</code> package.</p> <pre><code>source jupyter_env/bin/activate\npip install jupyterlab\n</code></pre>"},{"location":"jupyter/#submission-script","title":"Submission script","text":"<p>The submission script we'll use to run jupyterlab is shown below. We'll go through each section in turn.</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=ops-jupyter\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --mem=2G\n#SBATCH --signal=USR2\n#SBATCH --cpus-per-task=1\n\nmodule load python/3.11.6-gcc-13.2.0\nsource jupyter_env/bin/activate\n\n# get unused socket per https://unix.stackexchange.com/a/132524\nreadonly IPADDRESS=$(hostname -I | tr ' ' '\\n' | grep '10.211.4.')\nreadonly PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\ncat 1&gt;&amp;2 &lt;&lt;END\n1. SSH tunnel from your workstation using the following command:\n\n   Linux and MacOS:\n   ssh -NL 8888:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   Windows:\n   ssh -m hmac-sha2-512 -NL 8888:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8888/lab?token=&lt;add the token from the jupyter output below&gt;\n\nWhen done using the notebook, terminate the job by\nissuing the following command on the login node:\n\n      scancel -f ${SLURM_JOB_ID}\n\nEND\n\njupyter-lab --port=${PORT} --ip=${IPADDRESS} --no-browser\n\nprintf 'notebook exited' 1&gt;&amp;2\n</code></pre> <p>The first section of this is the #SBATCH statements which specify the resources we're requesting. If you want to request more memory or CPU cores, you'll need to modify this section. For example, to use 8 CPU cores and 25GB of memory:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=jupyter\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --mem=2G\n#SBATCH --signal=USR2\n#SBATCH --cpus-per-task=1\n</code></pre> <p>Tip</p> <p>To effectively make use of multiple CPU cores in Python code, you will usually need to explicitly specify this in your Python code, e.g. by using the Numba package. If you're not sure that you'll use them, don't request multiple CPU cores - you'll tie up resources that other users might want to use, and it'll slow down allocation of resources for your job.</p> <p>Next, the Python module is loaded and the virtual environment we just created is activated.</p> <pre><code>module load python/3.11.6-gcc-13.2.0\nsource jupyter_env/bin/activate\n</code></pre> <p>The next block identifies the IP address of the allocated node and finds an unused port for Jupyterlab to use. It then prints information to the SLURM output file which will explain how set up the SSH tunnel and connect to your Jupyterlab session after you submit the job.</p> <pre><code># get unused socket per https://unix.stackexchange.com/a/132524\nreadonly IPADDRESS=$(hostname -I | tr ' ' '\\n' | grep '10.211.4.')\nreadonly PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\ncat 1&gt;&amp;2 &lt;&lt;END\n1. SSH tunnel from your workstation using the following command:\n\n   Linux and MacOS:\n   ssh -NL 8888:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   Windows:\n   ssh -m hmac-sha2-512 -NL 8888:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8888/lab?token=&lt;add the token from the jupyter output below&gt;\n\nWhen done using the notebook, terminate the job by\nissuing the following command on the login node:\n\n      scancel -f ${SLURM_JOB_ID}\n\nEND\n</code></pre> <p>The final section of the batch script runs jupyterlab using the identified IP address and port.</p> <pre><code>jupyter-lab --port=${PORT} --ip=${IPADDRESS} --no-browser\n</code></pre> <p>Submit the script using <code>sbatch</code>, wait for the job to start (use <code>squeue --me</code> to check the status of your jobs), and check the output file for connection information:</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch jupyter.sh\nSubmitted batch job 15244802\nk1234567@erc-hpc-login2:~$ cat slurm-15244802.out\n</code></pre> <p>On your laptop or desktop, start a new terminal session and run the ssh command given in your job's output, e.g. <code>ssh -NL 8888:erc-hpc-comp015:53723 k1234567@hpc.create.kcl.ac.uk</code>.</p> <p>Tip</p> <p>This will produce a message saying you may need to visit the e-Research Portal to authorise your SSH connection, but then no further output, even if successful. This is expected!</p> <p>Then in your web browser go to http://localhost:8888/lab?token=yourtokenhere, and you should see the Jupyter Lab interface.</p> <p>When you're finished working, close the browser tab, use <code>Ctrl+C</code> to close the SSH tunnel, then on the HPC run <code>scancel -f &lt;your job id&gt;</code>, as given in the output file of your job, to cancel the job's allocation and free up the resources.</p>"},{"location":"next-steps/","title":"Next steps","text":"<p>King's staff and students can request access to CREATE HPC as described here.</p> <p>You may also want to register a project. This gives you a project-specific scratch space as well as the option to request backed-up RDS storage. We encourage project registration as it helps us track the research impact of CREATE usage.</p>"},{"location":"next-steps/#transferring-files","title":"Transferring files","text":"<p>One of the easiest and quickest ways to transfer files is using <code>scp</code> (secure copy). Running <code>scp &lt;source&gt; &lt;target&gt;</code> on your local machine (i.e., laptop or desktop) allows you to copy files from a source on your local machine to a target destination on the HPC storage, or from the HPC back to your local machine. This is suitable for files up to 500GB.</p> <p>It is strongly recommended that you use an <code>~/.ssh/config</code> file as outlined on the Accessing CREATE page.</p> <p>For MacOS/Linux, the config file should look like this, with <code>&lt;your k-number&gt;</code> replaced by your real k-number:</p> <pre><code>Host create\n    Hostname hpc.create.kcl.ac.uk\n    User &lt;your k-number&gt;\n    PubkeyAuthentication yes\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>For Windows the config file should look like this, again with <code>&lt;your k-number&gt;</code> replaced by your real k-number:</p> <pre><code>Host create\n    Hostname hpc.create.kcl.ac.uk\n    MACs hmac-sha2-512\n    User &lt;your k-number&gt;\n    PubkeyAuthentication yes\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>Using the config file in this format means that to copy a file called <code>hello_world.sh</code> from your local machine to your home directory on CREATE would be as simple as running the following command from your local machine in the same directory the file was in:</p> <p>Linux and MacOS:</p> <pre><code>scp hello_world.sh create:/users/k1234567/hello_world.sh\n</code></pre> <p>Windows:</p> <pre><code>scp -o MACs=hmac-sha2-512 hello_world.sh create:/users/k1234567/hello_world.sh\n</code></pre> <p>To copy a file called <code>hello_er.out</code> from the your personal scratch space in <code>/scratch/users/</code> to the current working directory on your local machine, you would run the following command on your local machine:</p> <p>Linux and MacOS:</p> <pre><code>scp create:/scratch/users/k1234567/hello_er.out ./hello_er.out\n</code></pre> <p>Windows:</p> <pre><code>scp -o MACs=hmac-sha2-512 create:/scratch/users/k1234567/hello_er.out ./hello_er.out\n</code></pre> <p>Tip</p> <p>For other ways to transfer files to CREATE HPC, see the instructions here.</p>"},{"location":"next-steps/#getting-help","title":"Getting help","text":"<p>If you're not sure how to do something on the HPC, first check the docs as many common tasks are already covered here. If you don't find the answer to your question there, you can ask a question on the e-Research support forum or search to see if someone else has already asked it. If you're still stuck, you can submit a ticket requesting help by emailing support@er.kcl.ac.uk.</p>"},{"location":"parallel_jobs/","title":"Parallel jobs","text":"<p>One main advantage of using an HPC system is the ability to utilise its large compute power to run jobs in parallel.</p> <p>Important</p> <p>When considering running parallel jobs make sure to consult your application documentation to find out if it can be run in the parallel environment. Nowadays most applications will support some level of parallelism. Many scientific software tools will have <code>-p</code> or <code>-t</code> options to specify numbers of CPUs to be used when running in parallel. Applications that can make use of multiple nodes are less common.</p> <p>If the application does not support parallelism, requesting additional resources will not improve performance, and will likely lead to longer waiting times for your job to be scheduled. It also leads to resources being wasted as they are allocated to your job but are unused.</p> <p>If you request multiple CPUs/nodes for your job, it's a good idea to check how effectively your job uses them using <code>sacct</code>, as discussed in the job monitoring section. Most applications do not scale infinitely and will reach a point where the marginal impact of allocating more resources is minimal.</p> <p>There are four types of parallel execution we'll discuss here, but we won't cover all of them in detail:</p> <ul> <li>Shared Memory Parallelism (SMP) / Multithreading</li> <li>Distributed Memory Parallelism / Multiprocessing</li> <li>Job Arrays</li> <li>GPUs</li> </ul> <p>Before we delve deeper into each of these, let's clarify some terminology:</p> <p>Processes: A process is an instance of a running program. It has its own memory space and resources allocated by the operating system. Processes are independent of each other and do not share memory, except through inter-process communication mechanisms.</p> <p>Threads: Threads are units of execution (or CPU utilisation) within a process. A process can have multiple threads, and these threads share the same memory space. Threads within the same process can communicate directly with each other. Threads are often used to perform multiple tasks concurrently within a single program.</p> <p>Tasks / Jobs: In the context of SLURM and other job scheduling systems, a task / job typically refers to a unit of work that can be executed independently of other units of work. Each task may correspond to a single process or a group of processes, depending on how the job is configured. So far, each time we've submitted something to the queue, we've created one task / job.</p>"},{"location":"parallel_jobs/#multithreadedmulticore-smp-jobs","title":"Multithreaded/multicore (SMP) jobs","text":"<p>Shared Memory Parallelism, as the name suggests, is a form of parallelism where all parallel threads of execution have access to a block of shared memory. We can use this shared memory to store objects which multiple threads need access to and to allow the threads to communicate.</p> <p>This can also be referred to as multithreading as the initial single thread of a process forks into a number of parallel threads. This approach will generally use a library such as <code>OpenMP</code> (Open MultiProcessing), <code>TBB</code> (Threading Building Blocks), or <code>pthread</code> (POSIX threads).</p> <p>Parallel programming</p> <p>Writing our own code which makes use of parallelism is a complex topic and is beyond the scope of this training. We use some minimal examples to help explain how different models of parallelism work, but don't worry if you don't fully understand all the examples.</p> <p>As an example, let's run a minimal C parallel program, called <code>omp_hello.c</code>, that prints \"Hello World\" for a number of threads. This program is available at <code>/datasets/hpc_training/utils/omp_hello</code>.</p> <p>Example files</p> <p>Most of the example files we use in this section can be found on CREATE at <code>/datasets/hpc_training/</code>.</p> <p>We need to create a shell script that requests appropriate resources to run the program:</p> <pre><code>#SBATCH --job-name=omp_hello\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=2G\n#SBATCH -t 0-0:02 # time (D-HH:MM)\n\n/datasets/hpc_training/utils/omp_hello\n</code></pre> <p>Hint</p> <p>You can also request memory per cpu, rather than per node using the <code>--mem-per-cpu</code> option. <code>--mem</code> and <code>--mem-per-cpu</code> are mutually exclusive meaning you can use one, or the other in your resource request.</p> <p>We submit the job with the following command:</p> <pre><code>sbatch run_omp_hello.sh\n</code></pre> <p>One possible output would be:</p> <pre><code>Hello World from OpenMP thread 2 of 4\nHello World from OpenMP thread 3 of 4\nHello World from OpenMP thread 0 of 4\nHello World from OpenMP thread 1 of 4\n</code></pre> <p>Note here that the lines don't come out in any particular order - each time you run the program you might end up with a different result. This is because the program doesn't make any attempt to synchronise the printing of each line, it just executes all of them in parallel.</p> <p>Order of execution</p> <p>When using parallelism, we cannot rely on threads / processes reaching a particular line of code in any particular order, though each individual thread will still execute the order as expected. If we need things to happen in a particular order, for example if we wanted the threads to say hello in order, we need to force this using synchronisation.</p>"},{"location":"parallel_jobs/#python-example","title":"Python example","text":"<p>Let's consider the slightly more complex example of a Python script, called <code>squares_numba.py</code>, which calculates the square of numbers from one to one billion using multithreading - again with OpenMP, but this time via the Numba library.</p> <pre><code>import time\n\nimport numba\nimport numpy as np\n\n@numba.jit(parallel=True)\ndef calculate_squares(n):\n    squares = np.zeros(n)\n\n    # Square numbers in parallel using Numba\n    for i in numba.prange(n):\n        # print(\"Hello from thread\", numba.get_thread_id())\n        squares[i] = i ** 2\n\n    return squares\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n\n    squares = calculate_squares(1_000_000_000)\n\n    end_time = time.time()\n    print(\"Used {} threads\".format(numba.get_num_threads()))\n    print(\"Took {:.4f} seconds\".format(end_time - start_time))\n</code></pre> <p>The output should be something like:</p> <pre><code>Used 8 threads\nTook 1.6164 seconds\n</code></pre> <p>The key steps in this code are:</p> <ul> <li>Import necessary libraries: Time to measure the execution time, NumPy for numerical computation, and Numba for parallelisation.</li> <li>Define a function <code>calculate_squares()</code> which calculates the squares of numbers from one to one billion - computers can square numbers very quickly, so this needs to be a large number.</li> <li>Use Numba's <code>@jit</code> decorator to enable JIT compilation and the <code>parallel=True</code> option to enable parallel execution using OpenMP. Python libraries like Numba use OpenMP internally to parallelize computations. Numba uses a compiler called LLVM to compile our Python code - when we ask for parallel execution with <code>@jit(parallel=True)</code>, that then uses OpenMP just like we did in our C++ example.</li> <li>Using <code>numba.prange</code> for our loop instead of the usual <code>range</code> causes it to be executed in parallel across all available threads. Each thread will be given an approximately equal share of the loop iterations to execute.</li> <li>In the main block we call the <code>calculate_squares()</code> function and time how long it takes to run.</li> </ul> <p>To execute the above code on CREATE, we can use <code>run_squares_numba.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=squares_numba\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=2G\n#SBATCH -t 0-0:02 # time (D-HH:MM)\n\n# Load any required modules\nmodule load python/3.9.12-gcc-10.3.0  \n\n# Activate virtual environment\nsource numba_venv/bin/activate\n\npython squares_numba.py\n</code></pre> <p>But before we run that we'll need to install necessary packages (in this case Numpy and Numba) in a virtual environment using:</p> <pre><code>python -m venv numba_venv\nsource numba_venv/bin/activate\npip install numba numpy\n</code></pre> <p>We can then submit the job:</p> <pre><code>sbatch submit_squares_numba.sh\n</code></pre>"},{"location":"parallel_jobs/#array-jobs","title":"Array jobs","text":"<p>Array jobs are a feature provided by job schedulers like SLURM and offer a mechanism for submitting and managing collections of similar tasks (a task is equal to a single \"job\" from a \"job array\") quickly and easily. Each task in the job array runs independently. Job arrays are useful for running multiple instances of the same job with different input parameters or configurations. Tasks in a job array may or may not communicate with each other, depending on the specific requirements of the job.</p> <p>Unlike in MPI or multi-threading, parallelization is orchestrated at the level of the shell script rather than within your program (e.g. Python script). Each task in the job array represents an independent instance of the program, and the shell script manages the execution of these instances in parallel by iterating over the task indices and launching separate invocations of the program. This approach allows you to parallelize the execution of the program across multiple tasks without modifying the script itself, making it a flexible and convenient method for running parallel tasks on HPC systems.</p> <p>All jobs will have the same initial options (e.g. memory, number of cpus, runtime, etc.) and will run the same commands. Using array jobs is an easy way to parallelise your workloads, as long as the following is true:</p> <ul> <li>Each array task can run independently of the others and there are no dependencies between the   different components (embarassingly parallel problem).</li> <li>There is no requirement for all of the array tasks to run simultaneously.</li> <li>You can link the array task id (<code>SLURM_ARRAY_TASK_ID</code>) somehow to your data, or execution of your application.</li> </ul> <p>To define an array job you will be using an <code>--array=range[:step][%max_active]</code> option:</p> <ul> <li><code>range</code> defines index values and can consist of comma separated list and/or a range of values with a \"-\" separator, e.g. <code>1,2,3,4</code>, or <code>1-4</code> or <code>1,2-4</code></li> <li><code>step</code> defines the increment between the index values, i.e. <code>0-15:4</code> would be equivalent to <code>0,4,8,12</code></li> <li><code>max_active</code> defines number of simultaneously running tasks at any give time, i.e. <code>1-10%2</code> means only two array tasks can run simultaneously for   the given array job</li> </ul> <p>A sample array job is given below:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=array-sample\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --mem=1G\n#SBATCH -t 0-0:02 # time (D-HH:MM)\n#SBATCH --array=1-3\n\necho \"Array job - task id: $SLURM_ARRAY_TASK_ID\"\n</code></pre> <p>Submit the array job using:</p> <pre><code>sbatch submit_array.sh\n</code></pre> <p>Info</p> <p>When the job starts running a separate job id in the format <code>jobid_taskid</code> will be assigned to each of the tasks.</p> <p>As a result, the array job will produce a separate log file for each of the tasks, i.e. you will see multiple files in the <code>slurm-jobid_taskid.out</code> format.</p>"},{"location":"parallel_jobs/#a-more-realistic-example","title":"A more realistic example","text":"<p>For a more realistic example, let's revisit the Python script we used earlier to identify the most frequent words in a text file. It would be useful to able to run this on many input text files, without having to modify the Python script or manually submit a job for each input. This is a great usecase for array jobs.</p> <p>Here's our original submission script, which specifies a single text file as input.</p> <pre><code>#! /bin/bash -l\n\n#SBATCH --job-name=top_words\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2G\n#SBATCH -t 0-0:10 # time (D-HH:MM)\n\nmodule load python/3.11.6-gcc-13.2.0\nsource top_words_env/bin/activate\n\npython top_words.py paradise-lost.txt 20\n</code></pre> <p>There are multiple text files we can use as input in the <code>/datasets/hpc_training/DH-RSE/data/</code> folder. We'll use <code>ls</code> to create a list of these input files and save it to an file.</p> <pre><code>ls /datasets/hpc_training/DH-RSE/data/*.txt &gt; input_files.txt\n</code></pre> <p>We can now use the <code>head</code> and <code>tail</code> commands to pull out individual lines in the file. For example, to extract the third line:</p> <pre><code>head input_files.txt -n 3 | tail -n 1\n</code></pre> <p>The <code>head</code> command extracts the first n lines of a file (here n = 3). We use the pipe <code>|</code> to pass this output directly to the <code>tail</code> command, which extracts the last n lines of its input. Here we specify n = 1 to extract just the last line returned by <code>head</code>, which is the the third line of the input file.</p> <p>In the submission script, we can use the <code>$SLURM_ARRAY_TASK_ID</code> to extract the corresponding file name. Here's what our updated submission script looks like:</p> <pre><code>#! /bin/bash -l\n\n#SBATCH --job-name=top_words\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2G\n#SBATCH -t 0-0:10 # time (D-HH:MM)\n#SBATCH --array=1-10\n\nmodule load python/3.11.6-gcc-13.2.0\nsource top_words_env/bin/activate\n\ninput_file=`head input_files.txt -n $SLURM_ARRAY_TASK_ID | tail -n 1`\n\necho \"Analysing \"$input_file\n\npython top_words.py $input_file 20\n</code></pre> <p>Submit the array job with:</p> <pre><code>sbatch submit_top_words_array.sh\n</code></pre> <p>You can then monitor the jobs with <code>squeue --me</code>. Output files will be produced for each item in the array.</p>"},{"location":"parallel_jobs/#distributed-memory-parallelism-dmp-mpi-jobs","title":"Distributed Memory Parallelism (DMP) / MPI jobs","text":"<p>Sometimes you might want to utilise resources on multiple nodes simultaneously to perform computations. This is possible using a Message Passing Interface (MPI). In MPI, multiple independent processes run concurrently on separate nodes or processors. Each process has its own memory space. Communication between processes is achieved through explicit message passing (processes send and receive messages via MPI function calls).</p> <p>As mentioned earlier, requesting the resource by itself will not make your application run in parallel - the application has to support parallel execution.</p> <p>Message Passing Interface (MPI) is the most common library used by research software for parallel execution across processes.</p> <p>Although MPI programming is beyond the scope of this course, if your application uses, or supports MPI then it can be executed on multiple nodes in parallel. For example, given the following submission script:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=multinode-test\n#SBATCH --partition=cpu\n#SBATCH --nodes=2\n#SBATCH --ntasks=16\n#SBATCH --mem=2G\n#SBATCH -t 0-0:05 # time (D-HH:MM)\n\nmodule load openmpi/4.1.3-gcc-10.3.0-python3+-chk-version\n\nmpirun /datasets/hpc_training/utils/mpi_hello\n</code></pre> <p>Which would be submitted using:</p> <pre><code>sbatch test_mpi.sh\n</code></pre> <p>A sample output would be:</p> <pre><code>Hello world from process 11 of 16 on host erc-hpc-comp006\nHello world from process 2 of 16 on host erc-hpc-comp005\nHello world from process 15 of 16 on host erc-hpc-comp006\nHello world from process 13 of 16 on host erc-hpc-comp006\nHello world from process 12 of 16 on host erc-hpc-comp006\nHello world from process 1 of 16 on host erc-hpc-comp005\nHello world from process 14 of 16 on host erc-hpc-comp006\nHello world from process 0 of 16 on host erc-hpc-comp005\nHello world from process 3 of 16 on host erc-hpc-comp005\nHello world from process 9 of 16 on host erc-hpc-comp006\nHello world from process 7 of 16 on host erc-hpc-comp005\nHello world from process 6 of 16 on host erc-hpc-comp005\nHello world from process 10 of 16 on host erc-hpc-comp006\nHello world from process 8 of 16 on host erc-hpc-comp006\nHello world from process 4 of 16 on host erc-hpc-comp005\nHello world from process 5 of 16 on host erc-hpc-comp005\n</code></pre>"},{"location":"parallel_jobs/#exercises-parallel-jobs-and-benchmarking","title":"Exercises - parallel jobs and benchmarking","text":"<p>Work through the exercises in this section to practice submitting parallel jobs.</p>"},{"location":"rstudio/","title":"RStudio","text":"<p>RStudio provides an integrated development environment (IDE) for the R programming language. RStudio on CREATE HPC provides access to all the benefits of the HPC (multiple cores for parallel jobs, more memory than available on your computer) while also having the convenience of the RStudio graphical user interface.</p> <p>RStudio is available on CREATE through the modules system:</p> <pre><code>k1234567@erc-hpc-login2:~$ module spider rstudio\n\n---------------------------------------------------------------------------------------------------------------------------------------------\n  rstudio:\n---------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        rstudio/v2023.03.0_386-gcc-11.4.0-r-4.1.1-python3+-chk-version\n        rstudio/v2023.03.0_386-gcc-11.4.0-r-4.2.2-python3+-chk-version\n        rstudio/v2023.03.0_386-gcc-11.4.0-r-4.3.0-python3+-chk-version\n\n---------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"rstudio\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider rstudio/v2023.03.0_386-gcc-11.4.0-r-4.3.0-python3+-chk-version\n---------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Tip</p> <p>Note that there may be versions of RStudio available that use different versions of R. Make sure you choose the one you want to use. Since different minor releases of R use different library locations, you will need to install packages separately for each one.</p> <p>To run RStudio on the compute nodes and access their resources, it's necessary to submit a batch job and then create an SSH tunnel from your machine to the running process. To submit a batch job to run RStudio, the e-Research team provides a template script.</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=ops-rstudio\n#SBATCH --partition=cpu\n#SBATCH --ntasks=1\n#SBATCH --mem=2G\n#SBATCH --signal=USR2\n#SBATCH --cpus-per-task=1\n\nmodule load rstudio/v2023.03.0_386-gcc-11.4.0-r-4.3.0-python3+-chk-version\n\n# get unused socket per https://unix.stackexchange.com/a/132524\nexport PASSWORD=$(openssl rand -base64 15)\nreadonly IPADDRESS=$(hostname -I | tr ' ' '\\n' | grep '10.211.4.')\nreadonly PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\ncat 1&gt;&amp;2 &lt;&lt;END\n1. SSH tunnel from your workstation using the following command:\n\n   Linux and MacOS:\n   ssh -NL 8787:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   Windows:\n   ssh -m hmac-sha2-512 -NL 8787:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8787\n\n2. Login to RStudio Server using the following credentials:\n\n   user: ${USER}\n   password: ${PASSWORD}\n\nWhen done using the RStudio Server, terminate the job by:\n\n1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window)\n2. Issue the following command on the login node:\n\n      scancel -f ${SLURM_JOB_ID}\n\nEND\n\n# Create custom database config\nDBCONF=$TMPDIR/database.conf\nif [ ! -e $DBCONF ]\nthen\nprintf \"\\nNOTE: creating $DBCONF database config file.\\n\\n\"\necho \"directory=$TMPDIR/var-rstudio-server\" &gt; $DBCONF\nfi\n\nrserver --server-user ${USER} --www-port ${PORT} --server-data-dir $TMPDIR/data-rstudio-server \\\n--secure-cookie-key-file $TMPDIR/data-rstudio-server/secure-cookie-key \\\n--database-config-file=$DBCONF --auth-none=0 \\\n--auth-pam-helper-path=pam-env-helper\n\nprintf 'RStudio Server exited' 1&gt;&amp;2\n</code></pre> <p>We'll go through this script section by section. The first section of this is the #SBATCH statements which specify the resources we're requesting. If you want to request more memory or CPU cores, you'll need to modify this section. For example, to use 8 CPU cores and 25GB of memory:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=ops-rstudio\n#SBATCH --partition=cpu\n#SBATCH --ntasks=8\n#SBATCH --mem=25G\n#SBATCH --signal=USR2\n#SBATCH --cpus-per-task=1\n</code></pre> <p>Tip</p> <p>To effectively make use of multiple CPU cores in R code, you will usually need to explicitly set up R to use these, e.g. by using the <code>parallel</code> package or the <code>future</code> package. If you're not sure that you'll use them, don't request multiple CPU cores - you'll tie up resources that other users might want to use and it'll slow down allocation of resources for your job.</p> <p>Next, the RStudio module is loaded - make sure the module version here matches the R version you want to use and is one of the options available to you in the output of <code>module spider rstudio</code>.</p> <pre><code>module load rstudio/v2023.03.0_386-gcc-11.4.0-r-4.3.0-python3+-chk-version\n</code></pre> <p>The next block does some setup and prints some information to the SLURM output file which will explain how to connect to your RStudio session after you submit the job.</p> <pre><code># get unused socket per https://unix.stackexchange.com/a/132524\nexport PASSWORD=$(openssl rand -base64 15)\nreadonly IPADDRESS=$(hostname -I | tr ' ' '\\n' | grep '10.211.4.')\nreadonly PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\ncat 1&gt;&amp;2 &lt;&lt;END\n1. SSH tunnel from your workstation using the following command:\n\n   Linux and MacOS:\n   ssh -NL 8787:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   Windows:\n   ssh -m hmac-sha2-512 -NL 8787:${HOSTNAME}:${PORT} ${USER}@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8787\n\n2. Login to RStudio Server using the following credentials:\n\n   user: ${USER}\n   password: ${PASSWORD}\n\nWhen done using the RStudio Server, terminate the job by:\n\n1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window)\n2. Issue the following command on the login node:\n\n      scancel -f ${SLURM_JOB_ID}\n\nEND\n</code></pre> <p>The final section of the batch script finishes setup for RStudio and runs it using the <code>rserver</code> command.</p> <pre><code># Create custom database config\nDBCONF=$TMPDIR/database.conf\nif [ ! -e $DBCONF ]\nthen\nprintf \"\\nNOTE: creating $DBCONF database config file.\\n\\n\"\necho \"directory=$TMPDIR/var-rstudio-server\" &gt; $DBCONF\nfi\n\nrserver --server-user ${USER} --www-port ${PORT} --server-data-dir $TMPDIR/data-rstudio-server \\\n--secure-cookie-key-file $TMPDIR/data-rstudio-server/secure-cookie-key \\\n--database-config-file=$DBCONF --auth-none=0 \\\n--auth-pam-helper-path=pam-env-helper\n\nprintf 'RStudio Server exited' 1&gt;&amp;2\n</code></pre> <p>Submit the script using <code>sbatch</code>, wait for the job to start (use <code>squeue --me</code> to check the status of your jobs), and check the output file for connection information:</p> <pre><code>k1234567@erc-hpc-login2:~$ sbatch rstudio.sh\nSubmitted batch job 15244802\nk1234567@erc-hpc-login2:~$ cat slurm-15244802.out\n1. SSH tunnel from your workstation using the following command:\n\n   Linux and MacOS:\n   ssh -NL 8787:erc-hpc-comp015:53723 k1234567@hpc.create.kcl.ac.uk\n\n   Windows:\n   ssh -m hmac-sha2-512 -NL 8787:erc-hpc-comp015:53723 k1234567@hpc.create.kcl.ac.uk\n\n   and point your web browser to http://localhost:8787\n\n2. Login to RStudio Server using the following credentials:\n\n   user: k1234567\n   password: WYk6MgV9DUkv0/UewDp3\n\nWhen done using the RStudio Server, terminate the job by:\n\n1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window)\n2. Issue the following command on the login node:\n\n      scancel -f 15244802\n\nNOTE: creating /tmp/database.conf database config file.\n</code></pre> <p>On your laptop or desktop, start a new terminal session and run the ssh command given in your job's output, e.g. <code>ssh -NL 8787:erc-hpc-comp015:53723 k1234567@hpc.create.kcl.ac.uk</code>.</p> <p>Tip</p> <p>This will produce a message saying you may need to visit the e-Research Portal to authorise your SSH connection, but then no further output, even if successful. This is expected!</p> <p>Then in your web browser go to http://localhost:8787. You should see an RStudio login page. Log in with your k-number and the password given in your job's output, and you should see an RStudio window.</p> <p>When you're finished working, close the RStudio session by clicking the power button in the top right corner, use <code>Ctrl+C</code> to close the SSH tunnel, then on the HPC run <code>scancel -f &lt;your job id&gt;</code>, as given in the output file of your job, to cancel the job's allocation and free up the resources.</p>"},{"location":"singularity/","title":"Singularity","text":"<p>Containers have become popular in the last decade and continue doing so. They provide a method to package an application so it can be run, with its dependencies, isolated from other processes and independent from the host environment. In a nutshell they are encapsulations of application, or system environments.</p> <p>Singuarity is a container technology which has been popular in HPC environments due to its security model (no root on the host, no root inside the container). You can find out more information about Singularity here.</p> <p>Info</p> <p>Singularity is only available on the compute nodes. You will not be able to run these commands on the login nodes, so for this section you should start an interactive job using:</p> <pre><code>srun --partition cpu --reservation cpu_introduction --pty /bin/bash\n</code></pre> <p>Singularity is already on the path and you will be able to pull the prebuilt/3rd party containers from Docker/Singularity hubs using</p> <pre><code>k1234567@erc-hpc-login1:~$ singularity pull docker://sylabsio/lolcow\n</code></pre> <pre><code>INFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 16ec32c2132b done  \nCopying blob 5ca731fc36c2 done  \nCopying config fd0daa4d89 done  \nWriting manifest to image destination\nStoring signatures\n2023/11/09 20:14:31  info unpack layer: sha256:16ec32c2132b43494832a05f2b02f7a822479f8250c173d0ab27b3de78b2f058\n2023/11/09 20:14:32  info unpack layer: sha256:5ca731fc36c28789c5ddc3216563e8bfca2ab3ea10347e07554ebba1c953242e\nINFO:    Creating SIF file...\n</code></pre> <p>and execute/run them once they have been downloaded and stored on the filesystem</p> <pre><code>k1234567@erc-hpc-login1:~$ singularity run lolcow_latest.sif\n</code></pre> <pre><code>INFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /etc/localtime required more than 50 (77) bind mounts\n _____________________________\n&lt; Thu Nov 9 20:15:06 GMT 2023 &gt;\n -----------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\nINFO:    Cleaning up image...\n</code></pre> <p>Generally, the <code>run</code> command will run a default script (also known as an entrypoint) that has been defined within the container. If you want to execute a specific command within the container you can do so using the <code>exec</code> option</p> <pre><code>k1234567@erc-hpc-login1:~$ singularity exec lolcow_latest.sif cowsay \"Hello World\"\n</code></pre> <pre><code>INFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /etc/localtime required more than 50 (77) bind mounts\n _____________\n&lt; Hello World &gt;\n -------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\nINFO:    Cleaning up image...\n</code></pre> <p>You can also build the containers yourself, but for the moment you will have to perform the building outside CREATE environment. Once built, you can copy them over and use them without any issues.</p> <p>Warning</p> <p>Using third party containers is a great way to get started, but you need to make sure that the container is behaving as expected before using it in your work.</p>"},{"location":"singularity/#exercises","title":"Exercises","text":"<p>Work through the exercises here to test your understanding of how to use Singularity on CREATE HPC.</p>"},{"location":"slides/","title":"Introduction to High Performance Computing with CREATE","text":""},{"location":"slides/#welcome","title":"Welcome!","text":"<p> <p>This workshop is an introduction to using the CREATE High Performance Computing (HPC) system and is intended to give a basic overview of the tools available and how to use them.</p> <p>By the end of the training, you should be able to:</p> <ul> <li>Connect to CREATE HPC</li> <li>Manage your files and programs on CREATE</li> <li>Use modules to find and load the necessary software</li> <li>Submit jobs to the queue and check the results of submitted jobs</li> </ul> <p>The training material and slides are available online, so you don't need to take detailed notes!</p> <p></p>"},{"location":"slides/#overview","title":"Overview","text":"<ul> <li>What is CREATE?</li> <li>What is a high performance computing cluster?</li> <li>Connecting to the HPC cluster</li> <li>Navigating the HPC filesystem</li> <li>Using software on CREATE HPC</li> <li>Submitting jobs to the HPC cluster</li> </ul>"},{"location":"slides/#about-create","title":"About CREATE","text":"<p> <p>King's Computational Research, Engineering and Technology Environment (CREATE) is a tightly integrated ecosystem of research computing infrastructure hosted by\u00a0King\u2019s College London. It consists of:</p> <ul> <li> <p>CREATE Cloud: A private cloud platform to provide flexible and scalable hosting environments using virtual machines</p> </li> <li> <p>CREATE HPC: A high performance compute cluster with CPU and GPU nodes, fast network interconnects and shared storage, for large scale simulations and data analytics</p> </li> <li> <p>CREATE RDS: A very large, highly resilient storage area for longer term curation of research data</p> </li> <li> <p>CREATE TRE: Tightly controlled project areas making use of Cloud and HPC resources to process sensitive datasets (e.g. clinical PII)</p> </li> <li> <p>CREATE Web: A self-service web hosting platform for static content (HTML/CSS/JS) and WordPress sites</p> </li> </ul> <p></p>"},{"location":"slides/#what-is-a-high-perfomance-computing-hpc-cluster","title":"What is a High Perfomance Computing (HPC) cluster?","text":"<p>A clustered network of computers\u00a0used to tackle large scale analytical problems.</p> <p></p>"},{"location":"slides/#what-is-a-compute-node","title":"What is a compute node?","text":"<ul> <li>CPU: Central Processing Unit</li> <li>Core: individual processor</li> </ul>"},{"location":"slides/#allocating-resources","title":"Allocating resources","text":"<p> The cluster uses a job scheduler that is\u00a0designed to support different types of workloads, making\u00a0use of all available compute resources with as much efficiency as possible. </p> <p></p>"},{"location":"slides/#connecting-to-create-hpc","title":"Connecting to CREATE HPC","text":""},{"location":"slides/#ssh-keys","title":"SSH keys","text":"<ul> <li>SSH keys are used to authenticate users</li> <li>Create an SSH key pair: <code>ssh-keygen \u2013t rsa</code></li> <li>Private key proves your identity - do not share this!</li> <li> <p>Public key is for authorization</p> </li> <li> <p>MacOS and Linux devices: <code>~/.ssh/id_rsa.pub</code></p> </li> <li>Windows devices: <code>C:\\Users\\k1234567\\.ssh\\id_rsa.pub</code></li> </ul>"},{"location":"slides/#adding-your-public-key-to-the-e-research-portal","title":"Adding your public key to the e-Research portal","text":"<ul> <li>Go to https://portal.er.kcl.ac.uk/access/ssh</li> </ul>"},{"location":"slides/#connecting-via-ssh","title":"Connecting via SSH","text":"<p> <ul> <li>Use <code>ssh</code> to connect to the HPC login node</li> <li>MacOS/Linux users: <code>ssh k1234567@hpc.create.kcl.ac.uk</code></li> <li>Windows users: <code>ssh \u2013m hmac-sha2-512 k1234567@hpc.create.kcl.ac.uk</code></li> <li>You will be asked to confirm the fingerprint if this is the first time connecting</li> <li>Approve your multifactor authentication request:  https://portal.er.kcl.ac.uk/mfa</li> </ul> <p></p> <p></p>"},{"location":"slides/#understanding-the-hpc-filesystem","title":"Understanding the HPC filesystem","text":"<p> <ul> <li> <p>Every user has their own personal storage areas in the following locations:</p> <ul> <li>Personal home directory: <code>/users/k1234567</code></li> <li>Personal scratch directory: <code>/scratch/users/k1234567</code></li> </ul> </li> <li> <p>Where you are a member, additional shared group locations are available for use:</p> <ul> <li>Shared\u00a0project directories, for example: <code>/scratch/prj/my_lab_project</code></li> <li>Dataset groups, for example: <code>/datasets/hpc_training/</code></li> </ul> </li> </ul> <p></p>"},{"location":"slides/#where-am-i","title":"Where am I?","text":"<p>Your personal home directory is the default landing location when you first log into the HPC. The print working directory (<code>pwd</code>) command will display your current location in the terminal:</p> <pre><code>k1234567@erc-hpc-login2:~$ pwd\n/users/k1234567\n</code></pre>"},{"location":"slides/#useful-commands","title":"Useful commands","text":"<p> <ul> <li> <p>Listing directory\u00a0contents: <code>ls -l --all</code></p> </li> <li> <p>Creating an empty file: <code>touch example-file.txt</code></p> </li> <li> <p>Creating a new empty directory: <code>mkdir example-folder</code></p> </li> <li> <p>Change working directory location: <code>cd example-folder</code></p> </li> <li> <p>Show current folder location: <code>pwd</code></p> </li> </ul> <p></p>"},{"location":"slides/#useful-commands-for-working-with-files","title":"Useful commands for working with files","text":"<p> <ul> <li> <p>Creating and writing\u00a0to a file: <code>nano hello-world.txt</code></p> </li> <li> <p>Reading a file: <code>less hello-world.txt</code></p> </li> <li> <p>Copying a file: <code>cp hello-world.txt my-copy.txt</code></p> </li> <li> <p>Moving a file: <code>mv hello-world.txt example-folder</code></p> </li> <li> <p>Remove a file: <code>rm my-copy.txt</code></p> </li> </ul> <p></p>"},{"location":"slides/#directory-structure","title":"Directory structure","text":"<p> <p><code>/users/k1234567/some_dir/myfile.txt</code></p> <p><code>/scratch/prj/example-group-folder/</code></p> <p></p>"},{"location":"slides/#using-software-on-create-hpc","title":"Using software on CREATE HPC","text":""},{"location":"slides/#available-software","title":"Available software","text":"<p>Centrally managed software\u00a0can\u00a0be found through\u00a0environment modules.</p> <ul> <li> <p>Find software using:</p> <ul> <li><code>module avail &lt;package-name&gt;</code></li> </ul> </li> <li> <p>For additional information on a module:</p> <ul> <li><code>module spider &lt;package-name&gt;</code></li> <li><code>module whatis &lt;package-name&gt;</code></li> </ul> </li> </ul>"},{"location":"slides/#using-software-modules","title":"Using software modules","text":"<ul> <li>Loading a module: <code>module load &lt;package-name&gt;</code></li> <li>Loading a module also dynamically loads required environment variables and paths to executables</li> <li>Listing currently loaded software modules: <code>module list</code></li> <li>Unloading a module: <code>module rm &lt;package-name&gt;</code></li> </ul>"},{"location":"slides/#submitting-jobs-on-create-hpc","title":"Submitting jobs on CREATE HPC","text":""},{"location":"slides/#job-types","title":"Job types","text":""},{"location":"slides/#next-steps","title":"Next steps","text":"<ul> <li>Fill in the training feedback survey!</li> <li>Request continued access to CREATE HPC<ul> <li>Consider requesting an RDS project - this gives up to 5TB backed-up storage</li> </ul> </li> <li>Check the documentation: https://docs.er.kcl.ac.uk/</li> <li>Check out the user forum: https://forum.er.kcl.ac.uk/</li> </ul>"},{"location":"ssh_keys/","title":"SSH keys","text":""},{"location":"ssh_keys/#what-are-ssh-keys","title":"What are SSH keys?","text":"<p>SSH keys are a widely used method for securely accessing remote machines, which if configured correctly provide better security than a username and password. Many HPC systems require users to use SSH keys for secure access, rather than a username and password.</p> <p>TODO: expand this section!</p>"},{"location":"ssh_keys/#creating-an-ssh-key","title":"Creating an SSH key","text":"<p>To create an SSH key we use <code>ssh-keygen</code> - when it prompts us for a passphrase, this is the passphrase/password which will be used to secure your key.</p> <pre><code>ssh-keygen\n</code></pre> <pre><code>Generating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in ~/.ssh/id_rsa\nYour public key has been saved in ~/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:mgjcz9Wz/9geaYgQtcg4eJCtrAsgJFobmX+26ZeqqmM\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   o.o    .      |\n|..= .o.o o .     |\n|+. =..+ + .      |\n|+...+.o. o       |\n|o o..o oS o      |\n|. .. +o+ . + . . |\n| . ...=  .o . +  |\n|.E.   . o  . + . |\n|oo.....o    oo+  |\n+----[SHA256]-----+\n</code></pre> <p>As you can see in the output of <code>ssh-keygen</code>, our SSH key is composed of two parts: the identification or private key and the public key. Your private key is the part which proves your identity and should never be shared with anyone else. Your public key is the part which a machine or service uses to say that you are authorised to access it.</p> <p>The next step is to provide your public key to the system you want to access. For CREATE HPC, this is done by uploading it to the e-Research Portal. Other HPC systems will have their own ways of providing an SSH key. The documentation or onboarding instructions for your system should tell you how to do this.</p>"},{"location":"submitting_jobs/","title":"Submitting Jobs","text":""},{"location":"submitting_jobs/#the-scheduler","title":"The scheduler","text":"<p>HPC systems are usually composed out of large number of nodes and have large number of users simultaneously using the facility. How do we ensure that the available resources are managed properly? This is a job of the scheduler, which is a heart and soul of the system and it's resposible for managing the jobs that are run on the cluster.</p> <p>As an analogy, think of the scheduler as a waiter in busy restaurant. This hopefully will give you some idea why sometimes you have to wait for the job to run.</p> <p></p> <p>CREATE is using SLURM scheduler, which stands for Simple Linux Utility for Resource Management. SLURM is commonly used by other HPC systems as well. You may also encounter other job schedulers such as PBS. These work on similar principles, although the exact commands and terminology will be different.</p>"},{"location":"submitting_jobs/#partitions","title":"Partitions","text":"<p>You can think of partitions as queues - they reside over specific sets of resources and allow access to particular groups. Following the restaurant analogy, think of them as different sections of the restaurant and corresponding queues assigned to them.</p> <p>The public partitions available on CREATE HPC are:</p> <ul> <li><code>cpu</code>: Partition for cpu jobs</li> <li><code>gpu</code>: Partition for gpu jobs</li> <li><code>long_cpu</code> and <code>long_gpu</code>: Partitions for long running jobs. Requires justification and explicit permission to use</li> <li><code>interruptible_cpu</code> and <code>interruptible_gpu</code>: Partitions that use unused capacity on private servers</li> </ul> <p>In addition, specific groups/faculties have their own partitions on CREATE HPC that can only be used by members of those groups. The list of CREATE partitions and who can use them can be found in our documentation. Additional information about the resource constraints can be found here.</p> <p>You can get the list of partitions that are available to you via <code>sinfo --summarize</code> command:</p> <pre><code>k1234567@erc-hpc-login1:~$ sinfo --summarize\nPARTITION         AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST\ncpu*                 up 2-00:00:00        35/0/0/35 erc-hpc-comp[001-028,183-189]\ngpu                  up 2-00:00:00        13/6/0/19 erc-hpc-comp[030-040],erc-hpc-vm[011-018]\ninterruptible_cpu    up 1-00:00:00       20/65/0/85 erc-hpc-comp[041-047,058-109,128-133,135,137,139-151,153-154,157,179-180]\ninterruptible_gpu    up 1-00:00:00       24/17/2/43 erc-hpc-comp[048-057,110-127,134,170-178,190-194]\n</code></pre> <p>Any additional rows you see in the output of <code>sinfo</code> will be private partitions you have access to.</p> <p>Hint</p> <p><code>NODES(A/I/O/T)</code> column refers to nodes state in the form <code>allocated/idle/other/total</code>.</p>"},{"location":"submitting_jobs/#submitting-jobs_1","title":"Submitting jobs","text":"<p>In most cases you will be submitting non-interactive jobs, commonly referred to as batch jobs. For this you will be using the <code>sbatch</code> utility.</p> <p>To submit a job to the queue, we need to write a shell script which contains the commands we want to run. When the scheduler picks our job from the queue, it will run this script. There's several ways we could create this script on the cluster, but for short scripts it's often easiest to use a command line text editor to create it directly on the cluster. For more complex scripts you might prefer to write them on your computer and transfer them across, but it's relatively rare that job submission scripts get that complex.</p> <p>One common text editor that you should always have access to on systems like CREATE is <code>nano</code>:</p> <pre><code>nano test_job.sh\n</code></pre> <p>Nano is relatively similar to a basic graphical text editor like Notepad on Windows - you have a cursor (controlled by the arrow keys) and text is entered as you type. Once we're done, we can use <code>Ctrl + O</code> to save the file - at this point if we haven't already told Nano the filename it will ask for one. Then finally, <code>Ctrl + X</code> to exit back to the command line. If we ever forget these shortcuts, Nano has a helpful reminder bar at the bottom.</p> <p>If you find yourself doing a lot of text editing on the cluster, it may be worth learning to use a more advanced text editor like Vim or Emacs, but Nano is enough for most people.</p> <p>We are going to start with a simple shell script <code>test_job.sh</code> that will contain the commands to be run during the job execution:</p> <pre><code>#!/bin/bash -l\n\necho \"Hello World! \"`hostname`\nsleep 60\n</code></pre> <p>From the login node, submit the job to the scheduler using:</p> <pre><code>sbatch --partition cpu --reservation cpu_introduction test_job.sh\n</code></pre> <p>We are specifying the partition to use (<code>cpu</code>), and also the reservation (<code>cpu_introduction</code>). We have set up a reservation for this workshop to ensure we don't have to wait too long to be allocated resources. Outside of an organised workshop, you likely won't have a reservation. On CREATE HPC, we can also often get test jobs like these to run more quickly by using the <code>interruptible_cpu</code> queue. The interruptible queues make use of otherwise unused space on private nodes, but if the owner of the nodes wants to use them, your running jobs may be cancelled. It's useful for quick testing, but if you're going to use the interruptible queues for real jobs you need to make sure they can be safely cancelled and not lose progress - this is often done via checkpointing.</p> <p>Once the command is executed you should see something similar to:</p> <pre><code>k1234567@@erc-hpc-login1:~$ sbatch --partition cpu --reservation cpu_introduction test_job.sh\nSubmitted batch job 56543\n</code></pre> <p>Info</p> <p>If you do not define a partition during the job submission the default partition will be used, in this case <code>cpu</code>.</p> <p>The job id (<code>56543</code>) is a unique identifier assigned to your job and can be used to query the status of the job. We will go through it in the job monitoring section.</p> <p>Important</p> <p>When submitting a support request please provide relevant jobids of the failed, or problematic jobs.</p>"},{"location":"submitting_jobs/#interactive-jobs","title":"Interactive jobs","text":"<p>Sometimes you might need, or want to run things interactively, rather than submitting them as batch jobs. This could be because you want to debug or test something, or the application/pipeline does not support non-interactive execution. To request an interactive job via the scheduler use the <code>srun</code> utility:</p> <pre><code>srun --partition cpu --reservation cpu_introduction --pty /bin/bash -l\n</code></pre> <p>The request will go through the scheduler and if resources are available you will be placed on a compute node, i.e.</p> <pre><code>k1234567@erc-hpc-login1:~$ srun --partition cpu --reservation cpu_introduction --pty /bin/bash -l\nsrun: job 56544 queued and waiting for resources\nsrun: job 56544 has been allocated resources\nk1234567@erc-hpc-comp001:~$\n</code></pre> <p>To exit an interactive job, we use the Bash command <code>exit</code> - this exits the current shell, so if you're inside an interactive job it will exit that, if you're just logged in to one of the login nodes, it will disconnect your SSH session.</p> <p>Warning</p> <p>At the moment there are no dedicated partitions, or nodes for interactive sessions and those sessions share the resources with all of the other jobs. If there are no free resources available you request will fail.</p> <p>Running applications with Graphical User Interfaces (GUIs)</p> <p>To run an interactive job for an application with a Graphical User Interface (GUI), for example RStudio, you must enable 'X11 forwarding' and 'authentication agent forwarding' when you connect to CREATE:</p> <pre><code>ssh -XA hpc.create.kcl.ac.uk\n</code></pre> <p>Then request compute resources using <code>salloc</code> - once your resources have been allocated you can then connect to the node with a further <code>ssh</code> connection:</p> <pre><code>salloc &lt;parameters&gt;\nssh -X $SLURM_NODELIST\nxeyes\n</code></pre>"},{"location":"submitting_jobs/#job-monitoring","title":"Job monitoring","text":"<p>It is important to be able to see the status of your running jobs, or to find out information about completed, or failed jobs.</p> <p>To monitor the status of the running jobs use <code>squeue</code> utility. Without any arguments, the command will print queue information for all users, however you can use <code>--me</code> parameter to filter the list:</p> <pre><code>k1234567@erc-hpc-login1:~$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             56544       cpu     bash k1234567  R       6:41      1 erc-hpc-comp001\n</code></pre> <p>Info</p> <p>Job state is described in the <code>ST</code> column. For the full list of states please see squeue docs (JOB STATE CODES section).</p> <p>The most common codes that you might see are:</p> <ul> <li><code>PD</code>: Pending - Job is awaiting resource allocation.</li> <li><code>R</code>: Running - Job currently has an allocation.</li> <li><code>CG</code>: Completing - Job is in the process of completing. Some processes on some nodes may still be active.</li> <li><code>CD</code>: Completed - Job has terminated all processes on all nodes with an exit code of zero.</li> </ul> <p>For jobs that have finished, you can use <code>sacct</code> utility to extract the relevant information.</p> <pre><code>sacct -j 56543\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n56543        test_job.+        cpu        kcl          1  COMPLETED      0:0\n56543.batch       batch                   kcl          1  COMPLETED      0:0\n</code></pre> <p>The above shows the default information.</p> <p>You can use <code>--long</code> option to display all of the stored information, or alternatively you can customise your queries to display the information that you specifically looking for by using <code>--format</code> parameter:</p> <pre><code>sacct -j 13378473 --format=ReqMem,AllocNodes,AllocCPUS,NodeList,JobID,Elapsed,State\n    ReqMem AllocNodes  AllocCPUS        NodeList        JobID    Elapsed      State\n---------- ---------- ---------- --------------- ------------ ---------- ----------\n    1000Mc          1          1         noded19 13378473       00:00:01  COMPLETED\n    1000Mc          1          1         noded19 13378473.ba+   00:00:01  COMPLETED\n</code></pre> <p>For the list of available options please see the job accounting fields in the <code>sacct</code> documentation.</p> <p>Check how efficiently your job used its resources</p> <p><code>sacct</code> can be used to check how efficiently your job used the resources you requested. For example, you can use the option <code>--format=JobID,JobName,Timelimit,Elapsed,CPUTime,ReqCPUS,NCPUS,ReqMem,MaxRSS</code> to get information on the maximum memory usage, total elapsed time, and CPU time used by your job.</p> <pre><code>k1234567@erc-hpc-login1:~$ sacct -j 8328 --format=JobID,JobName,Timelimit,Elapsed,CPUTime,ReqCPUS,NCPUS,ReqMem,MaxRSS\nJobID           JobName  Timelimit    Elapsed    CPUTime  ReqCPUS      NCPUS     ReqMem     MaxRSS\n------------ ---------- ---------- ---------- ---------- -------- ---------- ---------- ----------\n8328           hellowor   00:02:00   00:00:07   00:00:28        4          4         2G\n8328.ba+          batch              00:00:07   00:00:28        4          4               325772K\n</code></pre> <p>The example job above requested 2GB of memory but only used about 0.33 GB, and requested up to 2 minutes but only took 7 seconds.</p> <p>You should look at resource usage of your jobs and use this to guide the resources you request for similar jobs in the future. Jobs that request lower resources will likely be scheduled faster. Requesting only the resources you need also ensures that the HPC resources are used efficiently. Requesting more CPUs or memory than you need can stop other people's jobs running and lead to significant HPC resources sitting idle.</p>"},{"location":"submitting_jobs/#cancelling-jobs","title":"Cancelling jobs","text":"<p>You can cancel running, or queued job using <code>scancel</code> utility. You can cancel specific jobs using their <code>jobid</code></p> <pre><code>k1234567@erc-hpc-login1:~$ scancel 56544\n</code></pre> <p>If you want to cancel all of your jobs you can add the <code>--user</code> option</p> <pre><code>k1234567@erc-hpc-login1:~$ scancel --user k1234567\n</code></pre>"},{"location":"submitting_jobs/#choosing-the-resources","title":"Choosing the resources","text":"<p>Jobs require resources to be defined, e.g. number of nodes, cpus, amount of memory or the runtime. Defaults, such as 1 day runtime, 1 core, 1 node, etc are provided for convenience, but in most cases they will not be sufficient to accomodate more intensive jobs and explicit request has to be made for more.</p> <p>Warning</p> <p>If you do not request enough resources and your job exceeds the allocated amount, it will be terminated by the scheduler.</p> <p>The resources can be requested by passing additional options to the <code>sbatch</code> and <code>srun</code> commands. In most cases you will be using the following parameters to define the resources for your job:</p> <ul> <li>partition: <code>--partition</code>, or <code>-p</code> defines which partition, or queue your job will be targeting</li> <li>memory: <code>--mem</code> defines how much memory your job needs per allocated node</li> <li>tasks: <code>--ntasks</code>, or <code>-n</code> defines how many tasks your job needs</li> <li>nodes: <code>--nodes</code>, or <code>-n</code> defines how many nodes your job requires</li> <li>cpus per task: <code>--cpus-per-task</code>, defines how many cpus per task are needed</li> <li>runtime: <code>--time</code>, or <code>-t</code> defines how much time your job needs to run (in the <code>D-HH:MM</code> format)</li> <li>reservation: <code>--reservation</code> asks the scheduler to allocate your job to some pre-existing reserved space</li> </ul> <p>For a full list of options please see sbatch documentation.</p> <p>You can provide those options as arguments to the <code>sbatch</code>, or <code>srun</code> commands, i.e.</p> <pre><code>sbatch --job-name test_job --partition cpu --reservation cpu_introduction --ntasks 1 --mem 1G --time 0-0:2 test_job.sh\n</code></pre> <p>however that can be time consuming and prone to errors. Luckily you can also define those resource requirements in your submission scripts using <code>#SBATCH</code> tags. The sample job from the previous section will look like:</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=hello-world\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --mem=1G\n#SBATCH -t 0-0:2 # time (D-HH:MM)\n\necho \"Hello World! \"`hostname`\nsleep 60\n</code></pre> <p>Info</p> <p>In bash, and other shell scripting languages <code>#</code> is a special character usually representing comment (<code>#!</code> is an exception used to define the interpreter that the script will be executed with) and is ignored during the execution. For information on special characters in bash please see here.</p> <p>Hint</p> <p>You can specify SLURM options using two slightly different formats: 1. <code>--partition cpu</code> 1. <code>--partition=cpu</code> Here we have have used the first version on the command line and the second inside the submission script, but this isn't necessary. Either format can be used in either context.</p> <p><code>#SBATCH</code> is a special tag that will be interpreted by SLURM (other schedulers utilise similar mechanism) when the job is submitted. When the script is run outside the scheduler it will be ignored (becuse of the <code>#</code> comment). This is quite useful, as it means the script can be executed outside the scheduler control and will run successfully.</p> <p>Hint</p> <p>When requesting resources try to request them close to what your job needs, rather than requesting the maximum. Think back to the restaurant analogy - it's easier to find a table for a group of two people than a group of eight, so your job will likely be scheduled sooner. This also ensures that the HPC resources are being used efficiently.</p>"},{"location":"submitting_jobs/#using-gpus","title":"Using GPUs","text":"<p>GPUs are becoming increasingly widely used in research software applications, especially for machine learning and AI approaches.</p> <p>GPUs are very good at certain types of calculations, and can have &gt;10k cores each so can run many of these calculations in parallel. However, GPUs are not the best option for all tasks. GPUs are very bad at things that aren't these types of calculations, and typically have much smaller memory. In addition, GPU programming can be complex.</p> <p>You can request GPUs using the <code>--gres</code> option to SLURM. In the submission script below, <code>--gres gpu:1</code> requests one GPU. On CREATE HPC, you also need to use the <code>gpu</code> or <code>interruptible_gpu</code> partition. The <code>nvidia-smi</code> command prints some information about the GPUs allocated to the job.</p> <p>Hint</p> <p>You can request <code>X</code> gpus (up to 4) using <code>--gres gpu:X</code></p> <pre><code>#SBATCH --job-name=gpu-job\n#SBATCH --partition=interruptible_gpu\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4G\n#SBATCH -t 0-0:02 # time (D-HH:MM)\n#SBATCH --gres gpu:1\n\nnvidia-smi --id=$CUDA_VISIBLE_DEVICES\n</code></pre> <p>Submit this job using:</p> <pre><code>sbatch test_gpu.sh\n</code></pre> <p>A sample output would be:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla K40c          On   | 00000000:08:00.0 Off |                    0 |\n| 23%   32C    P8    23W / 235W |      0MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>Advanced resource requirements</p> <p>In some situations you might want to request specific hardware, such as chipset or fast network interconects. This can be achived with the use of <code>--constrain</code> option.</p> <p>To request a specific type of GPU <code>a100</code> you would use</p> <pre><code>#SBATCH --constrain=a100\n</code></pre> <p>or to request a specific type of processor/architecture you would use</p> <pre><code>#SBATCH --constrain=haswell\n</code></pre>"},{"location":"submitting_jobs/#job-log-files","title":"Job log files","text":"<p>By default the log files will be placed in the directory you have made your submission from (i.e. current working directory) in the format of <code>slurm-jobid.out</code>. Both stdout and stderr streams will be redirected from the job to that file. These log files are important as they will give you clues about the execution of your application in particular why it has failed.</p> <p>You can modify this to suit your needs by explicitly defining different path</p> <pre><code>#SBATCH --output=/scratch/users/%u/%j.out\n</code></pre> <p>You can also separate the stdout and stderr into separate log files</p> <pre><code>#SBATCH --output=/scratch/users/%u/%j.out\n#SBATCH --error=/scratch/users/%u/%j.err\n</code></pre> <p>Info</p> <p><code>%u</code> and <code>%j</code> are replacement symbols (representing username and job id) that will be replaced with actual values once the job is submitted. Please see file patterns section for details.</p>"},{"location":"submitting_jobs/#a-more-realistic-example","title":"A more realistic example","text":"<p>Let's work through a more realistic example.</p> <p>Here's a Python script which takes a text file as input and identifies the most common words in the file. The text is split into tokens, and punctuation and the most common English words are removed. The script then generates a wordcloud of the most common words, and saves it to a .png file. Finally, the top N words are output in a text file.</p> <pre><code>import nltk\nimport wordcloud\nimport pandas as pd\nimport os\nimport sys\n\nnltk.download('punkt_tab')\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# process command line arguments\nfile_name = sys.argv[1]\ntop_N = int(sys.argv[2])\n\nfile_base, ext = os.path.splitext(os.path.basename(file_name))\n\n# read in text\nwith open(file_name, encoding='utf-8-sig') as f:\n    txt = f.read().lower()\n\n# split text into tokens\ntokens = nltk.word_tokenize(txt)\n# remove punctuation and common words\npunct = punctuation + \"\u201c\" + \"\u201d\" + \"\u2019\"\nenglish_stop_words = set(stopwords.words('english'))\ntokens = [word for word in tokens if \n                     word not in english_stop_words and\n                     word not in punct]\n\n# create wordcloud and save to file\nwc = wordcloud.WordCloud()\nwc.generate(\",\".join(tokens))\nwc.to_file(file_base + '-wordcloud.png')\n\n# write csv file of top N tokens\ntoken_distribution = nltk.FreqDist(tokens)\ntop_tokens = pd.DataFrame(token_distribution.most_common(top_N),\n                            columns=['Word', 'Frequency'])\n\ntop_tokens.to_csv(file_base + '-top.csv')\n</code></pre> <p>The script can be run as follows, specifying the input text file and the number of top words to return:</p> <pre><code>python top_words.py paradise-lost.txt 20\n</code></pre> <p>To run this script, we need to first install the Python packages it uses. Let's create a new Python virtual environment to install them.</p> <pre><code>module load python/3.11.6-gcc-13.2.0\npython -m venv top_words_env\n</code></pre> <p>The packages we need to install are: <code>nltk</code>, for natural language processing; <code>wordcloud</code>, to create the wordcloud; and <code>pandas</code>, to create the table of most common words and their frequencies.</p> <pre><code>source top_words_env/bin/activate\npip install nltk pandas wordcloud\n</code></pre> <p>In the submission script for this job, we need to load the <code>python</code> module and activate the virtual environment. We then run the script, specifying the input text file and number of top words to return.</p> <pre><code>#! /bin/bash -l\n\n#SBATCH --job-name=top_words\n#SBATCH --partition=cpu\n#SBATCH --reservation=cpu_introduction\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2G\n#SBATCH -t 0-0:10 # time (D-HH:MM)\n\nmodule load python/3.11.6-gcc-13.2.0\nsource top_words_env/bin/activate\n\npython top_words.py paradise-lost.txt 20\n</code></pre> <p>Submit the job using:</p> <pre><code>sbatch submit_top_words.sh\n</code></pre> <p>Once it finishes, examine the output using <code>cat</code> or <code>less</code> to look at the contents of <code>paradise-lost-top.csv</code>.</p>"},{"location":"submitting_jobs/#exercises-submitting-jobs","title":"Exercises - submitting jobs","text":"<p>Work through the exercises in this section to practice submitting and debugging jobs.</p>"},{"location":"unix_commands/","title":"Basic Unix Commands","text":"<p>Unix is the name of a family of operating systems (OS) that support multi-tasking and multiple users. Unix commands allow you to interact with the OS and applications on it. Sometimes a Unix like operating system is deployed without a graphical user interface (GUI). Therefore, the only way to interact with the OS is through commandline. Furthermore, some applications do not have a GUI and can only be invoked through the command line.</p> <p>Below is a list of basic commands and an explanation for each command. Please note that some of these commands require one or more arguments while others do not need any.  </p> <p>Some commands can change behaviour or output based on switches provided. A switch is usually a letter or a word preceded by a - dash or two dashes -- respectively.</p> <ul> <li>cd<ul> <li>Changes the current directory that you are in.</li> <li>Examples:<ul> <li><code>cd dir1</code> # changes current directory to dir1</li> <li><code>cd</code> # changes current directory to your home directory</li> </ul> </li> <li>Notes:<ul> <li>Please see the notes on relative and absolute paths below.</li> </ul> </li> </ul> </li> <li>pwd<ul> <li>Prints the current directory you are in.</li> </ul> </li> <li>mkdir<ul> <li>Creates a new directory</li> <li>Examples:<ul> <li><code>mkdir test_dir</code> # Creates a directory with test_dir as its name</li> </ul> </li> </ul> </li> <li>mv<ul> <li>Moves file(s) or directories from a source path to a destination path.</li> <li>Examples:<ul> <li><code>mv test_file test_dir/</code> # Moves test_file to test_dir</li> </ul> </li> </ul> </li> <li>cp<ul> <li>Copies file(s) from a source path to a destination path.</li> <li>Examples:<ul> <li><code>cp test_file test_dir/</code> # Copies test_file to test_dir</li> </ul> </li> </ul> </li> <li>rm<ul> <li>Removes file(s).</li> <li>Examples:<ul> <li><code>rm test_file</code> # Removes test_file if it exists</li> </ul> </li> </ul> </li> <li>rmdir<ul> <li>Removes directory</li> <li>Examples:<ul> <li><code>rm test_dir</code> # Removes test_dir if it exists and is empty</li> </ul> </li> </ul> </li> <li> <p>tail</p> <ul> <li>Shows the last 10 lines from any text file.</li> <li>Examples:<ul> <li><code>tail some_file.txt</code> # Shows the last 10 lines of some_file.txt</li> <li><code>tail -f some_file.txt</code> # Shows the last 10 lines of some_file.txt and keeps track of changes</li> </ul> </li> </ul> </li> <li> <p>ls</p> <ul> <li>Lists the contents of the current directory.</li> <li>Examples:<ul> <li><code>ls -l</code> # Lists the contents of current directory in long format</li> <li><code>ls -a</code> # Lists the contents of current directory including hidden files.</li> </ul> </li> </ul> </li> <li>whoami<ul> <li>Prints your username.</li> </ul> </li> <li>history<ul> <li>Prints a list of commands executed by you.</li> </ul> </li> <li>groups<ul> <li>Prints the list of groups your user is a member of.</li> </ul> </li> <li>clear<ul> <li>Clears command line screen, useful when logs or warnings have filled up the screen.</li> </ul> </li> <li>last<ul> <li>Prints a history of logins to the system.</li> </ul> </li> <li>ps<ul> <li>Prints a list of existing processes of current user.</li> </ul> </li> <li>top<ul> <li>Shows a live list of tasks and resource usage.</li> </ul> </li> <li>man<ul> <li>Shows the manual of a specific command.</li> <li>Examples:<ul> <li><code>man tail</code> # Shows the manual for command tail</li> </ul> </li> </ul> </li> </ul>"},{"location":"unix_commands/#notes-on-relative-and-absolute-paths-in-unix","title":"Notes on Relative and Absolute Paths in Unix","text":"<ul> <li> <p>Some Unix commands accept paths as arguments. Paths can be defined as Relative or Absolute. Absolute paths start from the root of the file system with a forward slash / .Relative paths use the current working directory as a refrence and navigate the path from there. For instance, if you are in your home directory and wish to navigate to a directory called test_dir (which is in your home directory), you can use the <code>cd test_dir</code> command.  </p> </li> <li> <p>If you are in your home directory and wish to navigate to a directory that is not present there, you can use ../ (two full-stops followed by a forward slash) in the path to go one level up. For instance <code>cd ../some_dir/</code> would take you one level up from where you are. You can use as many ../ as the number of levels you need to go up in a relative path.</p> </li> </ul> <p>For example, if you are in a directory called <code>some_dir/</code> inside your home directory:</p> <pre><code>/users/k1234567\n\u251c\u2500\u2500 some_dir/            &lt;-- current location\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nested_dir/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 myfile.txt\n\u2514\u2500\u2500 test_dir/\n</code></pre> <p><code>cd nested_dir</code> will take you into <code>nested_dir/</code>.</p> <pre><code>/users/k1234567\n\u251c\u2500\u2500 some_dir/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nested_dir/      &lt;-- current location\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 myfile.txt\n\u2514\u2500\u2500 test_dir/\n</code></pre> <p>From there, <code>cd ..</code> will take you back to <code>some_dir/</code>. The command <code>cd ../../test_dir</code> will take you straight from <code>nested_dir/</code> to the <code>test_dir/</code> directory that is inside your home directory without any intermediate steps.</p> <pre><code>/users/k1234567\n\u251c\u2500\u2500 some_dir/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nested_dir/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 myfile.txt\n\u2514\u2500\u2500 test_dir/            &lt;-- current location\n</code></pre> <p>You could also move to these directories by using their absolute paths, e.g. <code>cd /users/k1234567/test_dir/</code>.</p>"},{"location":"using_software/","title":"Accessing software","text":"<p>HPC systems tend to provide software packages for the users. The number can be quite large and the software varied, especially on clusters where users come from different research backgrounds, such as bioinformatics, computer science, etc.</p> <p>By default no software is preloaded and to use any of the software packages available one must load them explicitly. This is driven by the challenges presented by</p> <ul> <li>multiple versions</li> <li>dependencies</li> <li>incompatibilities between different versions and packages</li> </ul> <p>Info</p> <p>On CREATE HPC, spack is used to install and manage the software on the platform.</p>"},{"location":"using_software/#environment-modules","title":"Environment Modules","text":"<p>Environment modules provide a way for a dynamic modification of a user's environment and address some of the challenges listed above. They are very useful in managing large numbers of applications and help to control multiple versions and the dependencies present.</p> <p>Each modulefile contains information needed to set up your environment for the application. In most cases it will alter, or define shell variables such as <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, etc.</p> <p>If there are specific dependencies, the modules will define and load them accordingly. For example, if a package relies on a specific compiler, the module file will load that compiler for you.</p> <p>To list available modules on the system use <code>module avail</code>. After you run it you should see something similar to the following output:</p> <pre><code>k1234567@erc-hpc-login1:~$ module avail\n\n----------------------------------------------------------------------------- /software/spackages_prod/modules/linux-ubuntu20.04-zen2 -----------------------------------------------------------------------------\n   alsa-lib/1.2.3.2-gcc-9.4.0                                                 perl-extutils-config/0.008-gcc-9.4.0\n   amdblis/3.0-gcc-9.4.0-python-3.8.12                                        perl-extutils-helpers/0.026-gcc-9.4.0\n   amdfftw/3.0-gcc-9.4.0-openmpi-4.1.1-python-3.8.12                          perl-extutils-installpaths/0.012-gcc-9.4.0\n   amdlibflame/3.0-gcc-9.4.0-python-3.8.12                                    perl-extutils-makemaker/7.24-gcc-9.4.0\n   anaconda3/2021.05-gcc-9.4.0                                                perl-extutils-pkgconfig/1.16-gcc-9.4.0\n   ant/1.10.7-gcc-9.4.0                                                       perl-file-listing/6.04-gcc-9.4.0\n   autoconf-archive/2019.01.06-gcc-9.4.0                                      perl-font-ttf/1.06-gcc-9.4.0\n   autoconf/2.69-gcc-9.4.0                                                    perl-gd/2.53-gcc-9.4.0-python-3.8.12\n   automake/1.16.3-gcc-9.4.0                                                  perl-html-parser/3.72-gcc-9.4.0\n   bazel/3.7.2-gcc-9.4.0-python-3.8.12                                        perl-html-tagset/3.20-gcc-9.4.0\n   bcftools/1.12-gcc-9.4.0-python-3.8.12                                      perl-http-cookies/6.04-gcc-9.4.0\n   bdftopcf/1.0.5-gcc-9.4.0                                                   perl-http-daemon/6.01-gcc-9.4.0\n   bedtools2/2.23.0-gcc-9.4.0-python-3.8.12                                   perl-http-date/6.02-gcc-9.4.0\n   berkeley-db/18.1.40-gcc-9.4.0                                              perl-http-message/6.13-gcc-9.4.0\n   binutils/2.37-gcc-9.4.0                                                    perl-http-negotiate/6.01-gcc-9.4.0\n   bismark/0.23.0-gcc-9.4.0-python-3.8.12                                     perl-io-html/1.001-gcc-9.4.0\n...\n</code></pre> <p>Info</p> <p>The version numbers you see might differ from the example shown here due to module updates since this training was written.</p> <p>You can run the above command yourself to see the full list. Use the arrow keys to scroll up and down the list, and <code>q</code> to go back to the command line when you're done.</p> <p>If you know the name, or parts of the name of the package that you want to use, you can search for it using <code>module spider</code>, e.g.</p> <pre><code>module spider python\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n  python:\n--------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        python/3.11.6-gcc-11.4.0\n        python/3.11.6-gcc-12.3.0\n        python/3.11.6-gcc-13.2.0\n     Other possible modules matches:\n        py-meson-python  py-mysql-connector-python  py-python-dateutil\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*python.*'\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"python\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider python/3.11.6-gcc-13.2.0\n--------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>To load a module use <code>module load</code> command</p> <pre><code>module load python/3.11.6-gcc-13.2.0\n</code></pre> <p>The above command loads a specific version of the application/module. If the explicit version is omitted during the <code>module load</code> request, i.e. <code>module load python</code>, the default version of the package will be loaded. The default version is marked by <code>D</code> (for default) in the output of <code>module avail</code>. It is better to load a specific version of the module rather than relying on the default versioning to avoid issues when the default version changes.</p> <p>Tip</p> <p>You might see different module versions that provide the same version of the application, but with different dependency versions, e.g. different versions of gcc. This is helpful for compatibility with other software that requires a specific gcc version.</p> <p>To list currently loaded modules use <code>module list</code></p> <pre><code>module list\n\nCurrently Loaded Modules:\n  1) bzip2/1.0.8-gcc-13.2.0     7) gdbm/1.23-gcc-13.2.0       13) zstd/1.5.5-gcc-13.2.0                     19) sqlite/3.43.2-gcc-13.2.0\n  2) libmd/1.0.4-gcc-13.2.0     8) libiconv/1.17-gcc-13.2.0   14) tar/1.34-gcc-13.2.0                       20) util-linux-uuid/2.38.1-gcc-13.2.0\n  3) libbsd/0.11.7-gcc-13.2.0   9) xz/5.4.1-gcc-13.2.0        15) gettext/0.22.3-gcc-13.2.0-libxml2-2.10.3  21) python/3.11.6-gcc-13.2.0\n  4) expat/2.5.0-gcc-13.2.0    10) zlib-ng/2.1.4-gcc-13.2.0   16) libffi/3.4.4-gcc-13.2.0\n  5) ncurses/6.4-gcc-13.2.0    11) libxml2/2.10.3-gcc-13.2.0  17) libxcrypt/4.4.35-gcc-13.2.0\n  6) readline/8.2-gcc-13.2.0   12) pigz/2.7-gcc-13.2.0        18) openssl/3.1.3-gcc-13.2.0\n</code></pre> <p>Important</p> <p>Although you can load the modules on the login nodes and they will be propagated to the scheduled jobs, you should load them in your job script as some of the software is optimised for specific processor architectures. You will also avoid conflicts and missing modules (in case you forget to load them before submitting the job).</p> <p>To remove, or unload a specific module use <code>module rm</code></p> <pre><code>module rm python/3.11.6-gcc-13.2.0\n</code></pre> <p>This will also unload any dependent modules required by the module you are unloading.</p> <p>Tip</p> <p>To remove all loaded modules use <code>module purge</code>.</p> <p>To find out more information about the module, use <code>module whatis</code></p> <pre><code>module whatis python/3.11.6-gcc-13.2.0\n</code></pre> <p>You should see information about the module</p> <pre><code>python/3.11.6-gcc-13.2.0                       : The Python programming language.\n</code></pre>"},{"location":"using_software/#exercises-modules","title":"Exercises - modules","text":"<p>To test your understanding of how to access software using modules, work through the exercises in this section.</p>"},{"location":"using_software/#installing-your-own-software","title":"Installing your own software","text":"<p>Although it's likely that many software packages are pre-installed on any HPC cluster, there is a chance that the package that you want to use is not installed and cannot be accessed as a module.</p> <p>You can of course build, or install your own packages. As you do not have root (administrative rights) on the nodes you won't be able to install them into system locations, or use system package managers (<code>apt</code>, <code>dpkg</code>) to perform the installations. You can however install into your own personal location, or the group shares - in essence anywhere you can write to. Build systems such as <code>cmake</code> or <code>automake</code> will have appropriate switches to define custom installation locations.</p> <p>Building and installing software can become complex, and is beyond the scope of this workshop. However, we will discuss a few other approaches to accessing software that you might find useful: Python virtual environments, Conda, and Singularity.</p> <p>Python virtual environments and Conda/Anaconda provide a way to extend functionality of an existing python installation and create isolated environments that can be used to install, or upgrade specific packages. Singularity is a tool for running software containers.</p>"},{"location":"using_software/#python-virtual-environments","title":"Python virtual environments","text":"<p>To create a Python virtual environment load the relevant python module and then execute <code>python3 -m venv envname</code> command replacing <code>envname</code> with the name of the environment you want to create:</p> <pre><code>k1234567@erc-hpc-login1:~$ module load python/3.11.6-gcc-13.2.0\nk1234567@erc-hpc-login1:~$ python3 -m venv myenv\n</code></pre> <p>With this, Python has created the <code>myenv</code> directory which now contains the required base files.</p> <p>Tip</p> <p>You can also specify the absolute/relative path to the environment if you do not want it created in the current directory.</p> <p>You only need to create the environment once. Once it has been created you do not need to run the command again (unless you need deleted the current one, or wish to create another one).</p> <p>To use the environment you have to activate it first by sourcing the activate script</p> <pre><code>k1234567@erc-hpc-login1:~$ source myenv/bin/activate\n(myenv) k1234567@erc-hpc-login1:~$\n</code></pre> <p>Important</p> <p>When activating the virtual environment you can use relative paths, but the activation has to be initiated from the right directory. Using full (absolute) path might be the safer alternative.</p> <p>The name of the venv should be prepended to your prompt indicating that the environment is active. From now on you can use the environment:</p> <ul> <li> <p>to install packages</p> <pre><code>(myenv) k1234567@erc-hpc-login2:~$ pip install pytest\nCollecting pytest\n  Downloading pytest-7.3.2-py3-none-any.whl (320 kB)\n    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 320.9/320.9 kB 12.8 MB/s eta 0:00:00\nCollecting tomli&gt;=1.0.0\n  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\nCollecting packaging\n  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 48.9/48.9 kB 2.5 MB/s eta 0:00:00\nCollecting iniconfig\n  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\nCollecting exceptiongroup&gt;=1.0.0rc8\n  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\nCollecting pluggy&lt;2.0,&gt;=0.12\n  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\nInstalling collected packages: tomli, pluggy, packaging, iniconfig, exceptiongroup, pytest\nSuccessfully installed exceptiongroup-1.1.1 iniconfig-2.0.0 packaging-23.1 pluggy-1.0.0 pytest-7.3.2 tomli-2.0.1\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.1.2\n[notice] To update, run: pip install --upgrade pip\n</code></pre> </li> <li> <p>or to run python scripts</p> <pre><code>(myenv) k1234567@erc-hpc-login1:~$ python /datasets/hpc_training/utils/helloworld.py\nHello World!\n</code></pre> </li> </ul> <p>To deactivate the environment use <code>deactivate</code> command</p> <pre><code>(myenv) k1234567@erc-hpc-login1:~$ deactivate\nk1234567@erc-hpc-login1:~$\n</code></pre> <p>You will see that the environment name has disappeared from the shell prompt.</p> Conda virtual environments <p>Conda is another tool for creating virtual environments. On CREATE, Conda is available via the <code>anaconda3</code> module.</p> <p>To use Conda, first load the module:</p> <pre><code>module load anaconda3/2022.10-gcc-13.2.0\n</code></pre> <p>You can then create a virtual environment and specify the packages you want to install into that environment. For example, to create an environment with a specific version of Python:</p> <pre><code>conda create --name python39-env python=3.9\n</code></pre> <p>The environment can be activated by running <code>conda activate</code> with the name of the env</p> <pre><code>k1234567@erc-hpc-login1:~$ conda activate python39-env\n(python39-env) k1234567@erc-hpc-login1:~$\n</code></pre> <p>and deactivated by running <code>conda deactivate</code></p> <pre><code>(python39-env) k1234567@erc-hpc-login1:~$ conda deactivate\nk1234567@erc-hpc-login1:~$\n</code></pre> <p>As with Python virtualenvs, note that the name of the active Conda environment is prepended to your prompt when the environment is active.</p> <p>Tip</p> <p>To prevent activation of the Conda base environment by default, which may conflict with other modules/packages, run <code>conda config --set auto_activate_base false</code></p>"},{"location":"using_software/#exercises-virtual-environments","title":"Exercises - virtual environments","text":"<p>To practice using Python virtual environments, work through the exercises in this section.</p>"}]}